{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTFw6VHL8T05"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "\n",
        "# Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Gurobi](https://img.shields.io/badge/Gurobi-8A2BE2.svg?style=flat&logo=gurobi&logoColor=white)](https://www.gurobi.com/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2508.00510-b31b1b.svg)](https://arxiv.org/abs/2508.00510)\n",
        "[![Research](https://img.shields.io/badge/Research-Disaster%20Economics-green)](https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Computational%20Economics-blue)](https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Input--Output%20Optimization-orange)](https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model\"** by:\n",
        "\n",
        "*   Surender Raj Vanniya Perumal\n",
        "*   Mark Thissen\n",
        "*   Marleen de Ruiter\n",
        "*   Elco E. Koks\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for evaluating the regional and macroeconomic consequences of disasters. It implements the updated MRIA model, a supply-constrained, multi-regional input-output model solved via linear programming. The goal is to provide a transparent, robust, and computationally efficient toolkit for researchers and policymakers to replicate, validate, and apply the MRIA framework to assess economic resilience and inform disaster risk mitigation strategies.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_full_experiment](#key-callable-run_full_experiment)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model.\" The core of this repository is the iPython Notebook `MRIA_model_macro_impact_disasters_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final execution of a full suite of robustness checks.\n",
        "\n",
        "Traditional input-output models often struggle to capture the supply-side shocks and logistical bottlenecks characteristic of disasters. This project implements the updated MRIA framework, which addresses these shortcomings by incorporating production capacity constraints, inter-regional substitution possibilities, and explicit logistical frictions.\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate and structure a complete set of multi-regional supply-use data.\n",
        "-   Transform tabular economic data into the precise numerical tensors required for optimization.\n",
        "-   Calibrate the model's key behavioral parameter (`alpha`) to ensure its baseline fidelity.\n",
        "-   Execute the core three-step optimization algorithm to simulate the post-disaster economic equilibrium.\n",
        "-   Conduct a full suite of analyses presented in the paper:\n",
        "    -   **Sensitivity Analysis:** Explore the trade-offs between production and trade flexibility.\n",
        "    -   **Criticality Analysis:** Identify systemically important economic sectors based on their irreplaceability.\n",
        "    -   **Incremental Disruption Analysis:** Trace the non-linear failure pathways of the economy under escalating stress.\n",
        "-   Execute a full suite of robustness checks to validate the stability of the model's conclusions.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in input-output economics and linear programming, providing a quantitative framework for simulating economic shocks.\n",
        "\n",
        "**1. Supply-Constrained, Multi-Regional Framework:**\n",
        "The model is built on a multi-regional Supply-and-Use Table (SUT) framework. Unlike traditional demand-driven models, the MRIA model's primary shock is a reduction in production capacity ($\\delta_{r,s} < 1$) in a disaster-affected region. Unaffected regions can compensate by increasing their own production, but this is limited by their own capacity and by logistical constraints on trade.\n",
        "\n",
        "**2. Three-Step Optimization Algorithm:**\n",
        "The post-disaster equilibrium is found by solving a sequence of three linear programs, which reflects a clear hierarchy of economic priorities:\n",
        "\n",
        "-   **Step 1: Minimize Rationing.** The primary goal is to satisfy as much final demand as possible, minimizing the direct welfare loss to consumers.\n",
        "    $$ \\min z_1 = \\sum_{r,p} v_{r,p} $$\n",
        "-   **Step 2: Minimize Economic Effort.** Given the unavoidable level of rationing found in Step 1, the model finds the most efficient (least-cost) way to organize production and trade to meet the remaining demand. The objective function includes a calibrated penalty ($\\alpha$) for using new or expanded trade routes.\n",
        "    $$ \\min z_2 = \\sum_{r,s} x_{r,s} + \\alpha \\sum_{r',r,p} t_{r',r,p} $$\n",
        "-   **Step 3: Quantify Production Equivalent of Rationing.** An analytical step to calculate the hypothetical production ($x'$) that would have been needed to satisfy the rationed demand, allowing for a comprehensive impact assessment.\n",
        "    $$ \\min z_3 = \\sum_{r,s} x'_{r,s} $$\n",
        "\n",
        "**3. Comprehensive Impact Assessment:**\n",
        "The total economic impact ($c$) is calculated as the sum of three distinct components, providing a holistic view of the disaster's costs:\n",
        "$$ c = \\sum_{r,p} (s^i_{r,p} - (s_{r,p} - v_{r,p})) + \\sum_{r,p} k_{r,p} + \\sum_{r,s} x'_{r,s} $$\n",
        "where the terms represent (1) the loss in efficiently supplied products, (2) the value of inefficient overproduction, and (3) the production equivalent of the welfare loss from rationing.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`MRIA_model_macro_impact_disasters_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Data Validation Pipeline:** A robust, modular system for validating the structure, content, and economic consistency of all input data.\n",
        "-   **Rigorous Preprocessing:** A deterministic pipeline for transforming tabular data into the precise `numpy` tensors required by the optimization engine.\n",
        "-   **Correct and Remediated Core Engine:** An accurate and professional-grade implementation of the three-step optimization algorithm using the `gurobipy` library, including a robust calibration routine.\n",
        "-   **Automated Analysis Orchestrators:** High-level functions that automate the execution of the Sensitivity, Criticality, and Incremental Disruption analyses with a single call.\n",
        "-   **Comprehensive Robustness Suite:** A full suite of advanced robustness checks to analyze the framework's sensitivity to parameters, data uncertainty (via Monte Carlo), and methodological choices.\n",
        "-   **Full Research Lifecycle:** The codebase covers the entire research process from data ingestion to final, validated results, providing a complete and transparent replication package.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Input Data Validation (Task 1):** The pipeline ingests four `pandas` DataFrames and a configuration dictionary, and rigorously validates their integrity.\n",
        "2.  **Data Preprocessing (Task 2):** It transforms the validated data into a structured set of `numpy` arrays and index mappings.\n",
        "3.  **Model Calibration (Task 3):** It systematically calibrates the `alpha` trade cost parameter to ensure the model replicates the baseline economy.\n",
        "4.  **Core MRIA Algorithm (Task 4):** It implements the central three-step optimization algorithm for a single disaster scenario.\n",
        "5.  **Sensitivity Analysis (Task 5):** It executes the core algorithm across a grid of 18 parameter settings to analyze resilience trade-offs.\n",
        "6.  **Criticality Analysis (Task 6):** It executes hundreds of simulations to stress-test each economic sector individually and calculate its systemic importance.\n",
        "7.  **Incremental Disruption Analysis (Task 7):** It executes dozens of simulations to trace the economy's non-linear response to escalating shocks.\n",
        "8.  **Orchestration & Robustness (Tasks 8-9):** Master functions orchestrate the main pipeline and the optional, full suite of robustness checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `MRIA_model_macro_impact_disasters_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 9 major tasks.\n",
        "\n",
        "## Key Callable: run_full_experiment\n",
        "\n",
        "The central function in this project is `run_full_experiment`. It orchestrates the entire analytical workflow, providing a single entry point for running the main analyses and the advanced robustness checks, all controlled by a single configuration dictionary.\n",
        "\n",
        "```python\n",
        "def run_full_experiment(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete MRIA research study, including primary and robustness analyses.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.8+\n",
        "-   A Gurobi license. Gurobi offers free academic licenses. The `gurobipy` package must be installed and the license configured.\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `gurobipy`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters.git\n",
        "    cd MRIA_model_macro_impact_disasters\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy \"gurobipy>=9.5\" tqdm\n",
        "    ```\n",
        "\n",
        "4.  **Install and configure Gurobi:**\n",
        "    Follow the instructions on the [Gurobi website](https://www.gurobi.com/documentation/) to install the Gurobi Optimizer and activate your license.\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires four `pandas` DataFrames with specific structures, which are rigorously validated by the first task.\n",
        "1.  `initial_production`: `MultiIndex('region', 'sector')`, column `'production_value'`.\n",
        "2.  `initial_trade`: `MultiIndex('origin_region', 'dest_region', 'product')`, column `'trade_value'`.\n",
        "3.  `supply_coefficients`: `MultiIndex('region', 'sector', 'product')`, column `'coefficient'`.\n",
        "4.  `use_coefficients`: `MultiIndex('dest_region', 'dest_sector', 'product', 'origin_region')`, column `'coefficient'`.\n",
        "\n",
        "The entire experiment is controlled by a single, comprehensive Python dictionary, `config`. A fully specified example is provided in the notebook.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `MRIA_model_macro_impact_disasters_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your four data `DataFrame`s and define your `config` dictionary. A complete template is provided.\n",
        "2.  **Execute Pipeline:** Call the master orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs all analyses enabled in the config dictionary.\n",
        "    full_results = run_full_experiment(\n",
        "        initial_production,\n",
        "        initial_trade,\n",
        "        supply_coefficients,\n",
        "        use_coefficients,\n",
        "        full_config\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned dictionary. For example, to view the criticality analysis results:\n",
        "    ```python\n",
        "    crit_results = full_results['main_analysis_results']['criticality_analysis_results']\n",
        "    print(crit_results.head())\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_full_experiment` function returns a single, comprehensive dictionary with two top-level keys:\n",
        "-   `main_analysis_results`: A dictionary containing the results of the primary analyses (Sensitivity, Criticality, Incremental Disruption) that were enabled in the config.\n",
        "-   `robustness_analysis_results`: A dictionary containing the results of the advanced robustness checks that were enabled in the config.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "MRIA_model_macro_impact_disasters/\n",
        "│\n",
        "├── MRIA_model_macro_impact_disasters_draft.ipynb  # Main implementation notebook   \n",
        "├── requirements.txt                                 # Python package dependencies\n",
        "├── LICENSE                                          # MIT license file\n",
        "└── README.md                                        # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the master `config` dictionary. Users can easily enable/disable any analysis and modify all relevant parameters, including:\n",
        "-   The `disruption_scenario` for the sensitivity analysis.\n",
        "-   The `parameter_grid` of resilience factors to test.\n",
        "-   The `disruption_magnitude` for the criticality analysis.\n",
        "-   The target sectors and `disruption_levels` for the incremental analysis.\n",
        "-   All parameters for the suite of robustness checks.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{perumal2025assessing,\n",
        "  title={Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model},\n",
        "  author={Perumal, Surender Raj Vanniya and Thissen, Mark and de Ruiter, Marleen and Koks, Elco E.},\n",
        "  journal={arXiv preprint arXiv:2508.00510},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model\".\n",
        "GitHub repository: https://github.com/chirindaopensource/MRIA_model_macro_impact_disasters\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Surender Raj Vanniya Perumal, Mark Thissen, Marleen de Ruiter, and Elco E. Koks for their insightful and clearly articulated research.\n",
        "-   Thanks to the developers of the scientific Python ecosystem (`numpy`, `pandas`, `scipy`) and the Gurobi team for their powerful optimization tools.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `MRIA_model_macro_impact_disasters_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "HzGu3LunZoIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model*\"\n",
        "\n",
        "Authors: Surender Raj Vanniya Perumal, Mark Thissen, Marleen de Ruiter, Elco E. Koks\n",
        "\n",
        "E-Journal Submission Date: 1 August 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2508.00510\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Disasters often impact supply chains, leading to cascading effects across regions. While unaffected regions may attempt to compensate, their ability is constrained by their available production capacity and logistical constraints between regions. This study introduces a Multi-Regional Impact Assessment (MRIA) model to evaluate the regional and macroeconomic consequences of disasters, capturing regional post-disaster trade dynamics and logistical constraints. Our findings emphasize that enhancing production capacity alone is inadequate; regional trade flexibility must also be improved to mitigate disaster impacts. At the regional level, disaster-affected areas experience severe negative impacts, whereas larger, export-oriented regions benefit from increased production activity. Additionally, we propose a sectoral criticality assessment alongside the more common sensitivity and incremental disruption analysis, which effectively identifies sectors with low redundancy while accounting for the potential for regional substitution in a post-disaster scenario."
      ],
      "metadata": {
        "id": "SNlb137r9HYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Summary of the Updated MRIA Model Paper\n",
        "\n",
        "#### **The Core Problem and Motivation**\n",
        "\n",
        "The authors address a fundamental challenge in disaster economics: accurately modeling the cascading macroeconomic impacts of localized disasters. Disasters create supply-side shocks that propagate through complex, inter-regional supply chains. The paper identifies key limitations in existing modeling paradigms:\n",
        "\n",
        "*   **Traditional Input-Output (IO) Models:** These are often too rigid and linear. They struggle to handle supply-side shocks and lack mechanisms for economic adaptation, such as increasing production in unaffected regions or substituting suppliers.\n",
        "*   **Computable General Equilibrium (CGE) Models:** While more sophisticated—incorporating price adjustments and substitution—they are data-intensive, computationally expensive, and often assume the economy rapidly finds a new equilibrium, which is a strong assumption in the immediate chaotic aftermath of a disaster.\n",
        "\n",
        "The paper positions its work as an advancement over a previous model (Koks & Thissen, 2016), aiming for a computationally tractable framework that incorporates realistic economic flexibilities without the full complexity of a CGE model.\n",
        "\n",
        "#### **The Proposed Solution - An Updated MRIA Model**\n",
        "\n",
        "The core of the paper is an updated Multi-Regional Impact Assessment (MRIA) model. From a computational and mathematical perspective, it is an **adaptive linear programming model** built upon a multi-regional Supply-and-Use Table (SUT) framework.\n",
        "\n",
        "The model's primary goal is to determine the new state of the economy (production levels, trade flows, and consumption) that satisfies post-disaster demand while minimizing overall economic disruption. The authors introduce two crucial innovations over the previous version:\n",
        "\n",
        "1.  **Explicit Inter-Regional Trade Flows:** The original model only calculated the *net* import requirements for a region. This updated version explicitly models the bilateral trade flows (`t_r',r,p`) of a specific product `p` from an origin region `r'` to a destination region `r`. This provides a far more granular and realistic view of supply chain re-routing.\n",
        "2.  **Logistical Constraints via a \"Trade Flexibility Factor\":** This is arguably the paper's most significant contribution. The authors recognize that post-disaster trade is not infinitely flexible; it is constrained by existing infrastructure and logistics. They introduce a **trade flexibility factor (`Φ`)**, which limits the increase in trade between any two regions to a certain multiple of the pre-disaster trade volume. This prevents unrealistic outcomes where regions with no prior trade relationship suddenly become major partners and correctly models the friction inherent in scaling up logistical capacity.\n",
        "\n",
        "#### **The Mathematical and Economic Framework**\n",
        "\n",
        "The MRIA model is formulated as a multi-step optimization problem:\n",
        "\n",
        "*   **Step 1: Minimize Rationing.** The model first solves to find the absolute minimum level of rationing (unmet final demand) possible, given the post-disaster production and trade constraints. This reflects the economic priority of satisfying consumer needs above all else.\n",
        "*   **Step 2: Minimize Production and Trade Costs.** Using the minimum rationing level from Step 1 as a fixed constraint, the model then solves a second optimization problem. The objective function is to **minimize total production and disaster-related trade**, weighted by a parameter `α` that represents the logistical cost of trade. This finds the \"least cost\" way for the economy to reorganize itself to meet the remaining demand.\n",
        "*   **Step 3: Quantify Rationing Cost.** The model performs a final calculation to determine the production equivalent of the rationed goods, providing a comprehensive measure of the total economic impact.\n",
        "\n",
        "The model's key constraints are:\n",
        "*   **Supply-Demand Balance:** For every product in every region, total supply (local production + imports) must be greater than or equal to total demand (intermediate use + final consumption + exports). The inequality allows for *inefficient production* (e.g., producing unwanted by-products to get a needed primary product), a subtle but realistic feature.\n",
        "*   **Production Capacity:** Post-disaster production in any sector is capped by its pre-disaster level, multiplied by a **production extension factor (`δ`)**. `δ < 1` simulates disruption, while `δ > 1` allows for increased production in unaffected sectors.\n",
        "*   **Trade Capacity:** As described above, post-disaster trade is capped by the pre-disaster level multiplied by the **trade flexibility factor (`Φ`)**.\n",
        "\n",
        "#### **Empirical Application and Analyses**\n",
        "\n",
        "The authors apply their model to a case study of the Netherlands, divided into 12 NUTS-2 regions. They simulate a 10% production disruption to all manufacturing sectors in the economically vital Zuid-Holland region and perform three distinct analyses:\n",
        "\n",
        "1.  **Sensitivity Analysis:** They systematically vary the production extension (`δ`) and trade flexibility (`Φ`) parameters to understand how the system's resilience changes. This creates a spectrum of scenarios from a \"rigid\" economy (low flexibility) to a \"flexible\" one (high flexibility).\n",
        "2.  **Criticality Analysis:** In a novel approach, they stress *every individual sector* in the economy and measure the resulting system-wide rationing. A sector is deemed \"critical\" if its disruption causes high levels of rationing, indicating a lack of viable substitutes across the multi-regional system. They compare this metric to standard indicators like economic output and Location Quotient (LQ).\n",
        "3.  **Incremental Disruption Analysis:** They analyze two key sectors and increase the disruption level from 1% to 100% to identify non-linear responses and tipping points. This reveals the thresholds at which the economic system's coping mechanisms become overwhelmed.\n",
        "\n",
        "#### **Key Findings and Results**\n",
        "\n",
        "The analyses yield several powerful insights:\n",
        "\n",
        "*   **Production and Trade Flexibility are Complements:** The model clearly shows that increasing production capacity is ineffective if logistical constraints prevent the new output from reaching the regions that need it. Both capabilities must be enhanced in tandem to build true economic resilience.\n",
        "*   **Disasters Create Winners and Losers:** While the disrupted region (Zuid-Holland) suffers significant losses, other major economic regions (e.g., Noord-Holland) experience an *increase* in production as they ramp up to fill the supply gap. This highlights the importance of a multi-regional perspective.\n",
        "*   **Criticality is a Function of Redundancy, Not Just Size:** The analysis identifies the Coke & Refined Petroleum sector as the most critical, not because it's the largest, but because its production is highly concentrated in one region with no substitutes. In contrast, the large Food & Beverages sector is less critical because its production is geographically dispersed, allowing for easy regional substitution. This demonstrates the superiority of their systemic, rationing-based criticality metric.\n",
        "*   **Economic Resilience Has Thresholds:** The incremental analysis reveals distinct phases of response. For a sector with substitutes, there is a \"zone of no rationing\" where the system absorbs the shock. This is followed by a \"zone of limited rationing\" and finally a \"rationing cascade\" where the system is overwhelmed and impacts multiply.\n",
        "\n",
        "#### **Conclusion and Implications**\n",
        "\n",
        "The paper concludes that the updated MRIA model is a robust tool for assessing disaster impacts. By explicitly modeling bilateral trade and, crucially, imposing logistical constraints, it provides a more realistic picture of post-disaster economic adaptation. The findings have direct policy relevance, suggesting that resilience strategies must focus on both production redundancy and logistical flexibility. The proposed criticality analysis offers a data-driven method for prioritizing the protection of key economic sectors and their supporting infrastructure. The authors acknowledge the model's static nature as a limitation and propose incorporating dynamic recovery processes and inventory management in future work."
      ],
      "metadata": {
        "id": "iGTq16X199XQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "up3lv7Xavq_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional\n",
        "#  Impact Assessment (MRIA) model\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Assessing the Macroeconomic Impacts of\n",
        "#  Disasters: an Updated Multi-Regional Impact Assessment (MRIA) model\" by\n",
        "#  Perumal et al. (2025). It delivers a robust, configuration-driven system for\n",
        "#  quantitative, system-level decision support, enabling the optimization of\n",
        "#  economic resilience by identifying and mitigating vulnerabilities arising from\n",
        "#  the interplay between production concentration and logistical constraints.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Multi-Regional, Multi-Sectoral Supply-Use economic modeling\n",
        "#  • Three-step linear programming algorithm for post-disaster simulation\n",
        "#  • Explicit modeling of production capacity constraints and trade flexibility\n",
        "#  • Endogenous rationing of final demand as a measure of welfare loss\n",
        "#  • Comprehensive economic impact accounting (supply loss, inefficiency, rationing)\n",
        "#\n",
        "#  Key Analytical Capabilities:\n",
        "#  • Sensitivity Analysis: Assesses impacts across a grid of resilience parameters.\n",
        "#  • Criticality Analysis: Identifies systemically important sectors based on their\n",
        "#    irreplaceability in the supply chain.\n",
        "#  • Incremental Disruption Analysis: Traces non-linear system responses and\n",
        "#    identifies critical failure thresholds.\n",
        "#  • Robustness Testing: Validates model conclusions against parameter and\n",
        "#    structural uncertainty using Monte Carlo and scenario-based methods.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Perumal, S. R. V., Thissen, M., de Ruiter, M., & Koks, E. E. (2025).\n",
        "#  Assessing the Macroeconomic Impacts of Disasters: an Updated Multi-Regional\n",
        "#  Impact Assessment (MRIA) model. arXiv preprint arXiv:2508.00510.\n",
        "#  https://arxiv.org/abs/2508.00510\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Core Data Handling and Numerical Operations ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Optimization Engine ---\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "# --- Concurrency for Performance ---\n",
        "import concurrent.futures\n",
        "\n",
        "# --- Standard Library Utilities ---\n",
        "import copy\n",
        "import itertools\n",
        "from typing import Any, Dict, List, Set, Tuple, TypedDict\n",
        "\n",
        "# --- Third-Party Utilities ---\n",
        "# For statistical analysis (Spearman correlation)\n",
        "from scipy.stats import spearmanr\n",
        "# For progress monitoring in long-running computations\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "Upw0C3g6vurj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "WSsR8jRmvwxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Discussion of the Inputs, Processes and Outputs (IPO) of Key Callables\n",
        "\n",
        "### **Task 1: Input Data Validation and Quality Assurance**\n",
        "\n",
        "**Callable: `run_input_validation_and_quality_assurance` (and its helpers)**\n",
        "*   **Inputs:** Four `pandas` DataFrames (`initial_production`, `initial_trade`, `supply_coefficients`, `use_coefficients`) and the master `config` dictionary.\n",
        "*   **Processes:** This orchestrator executes a battery of validation checks. It verifies the precise structure (index levels, names, dtypes) of the input DataFrames. It confirms that all categorical identifiers (region, sector, product codes) are from a consistent and valid set. It performs critical economic sanity checks, such as ensuring all monetary values are non-negative and that supply coefficients for active sectors sum to one ($\\sum_{p} C_{r,s,p} = 1.0$). It also validates the export feasibility constraint ($Export_r \\leq X_r$). Finally, it recursively validates the entire `config` dictionary against a predefined schema.\n",
        "*   **Outputs:** A tuple containing the validated, unique sets of region, sector, and product codes. If any check fails, it raises a descriptive `ValueError` or `TypeError`, halting the pipeline.\n",
        "*   **Transformation:** The primary transformation is one of validation and extraction. It distills the sets of unique identifiers from the raw data, which become the canonical dimensions for the entire model.\n",
        "*   **Role in Research Pipeline:** This function serves as the **Gatekeeper**. It is the first and most critical step, ensuring the integrity, consistency, and logical soundness of all inputs. It prevents the propagation of errors (\"garbage in, garbage out\") and ensures that the subsequent numerical models are built upon a valid foundation. It implements the implicit assumptions of a well-defined economic accounting system that underpins any input-output model.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 2: Data Preprocessing and Matrix Construction**\n",
        "\n",
        "**Callable: `preprocess_data_for_mria` (and its helpers)**\n",
        "*   **Inputs:** The four validated `pandas` DataFrames and the tuple of validated identifier sets from Task 1.\n",
        "*   **Processes:** This orchestrator transforms the tabular `pandas` data into the precise numerical tensors required by the optimization model. It first creates canonical, sorted mappings from string identifiers to integer indices. It then uses these mappings to construct dense `numpy` arrays for baseline production ($\\bar{x}_{r,s}$), baseline trade ($\\bar{t}_{r',r,p}$), supply coefficients ($C_{r,s,p}$), and use coefficients ($B_{d,s,p,o}$). It also initializes the final demand vectors to zero for the baseline scenario.\n",
        "*   **Outputs:** A single `PreprocessedData` TypedDict containing all the `numpy` arrays and index mappings.\n",
        "*   **Transformation:** This is a pure data transformation step. It converts structured, labeled, and potentially sparse tabular data into dense, unlabeled, and consistently ordered numerical tensors suitable for high-performance linear algebra and optimization.\n",
        "*   **Role in Research Pipeline:** This function is the **Translator**. It bridges the gap between the human-readable economic data and the abstract mathematical representation required by the Gurobi solver. It operationalizes the indices `r, s, p` used throughout the paper's equations.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 3: Model Calibration Implementation**\n",
        "\n",
        "**Callable: `_build_mria_lp_model`**\n",
        "*   **Inputs:** The `PreprocessedData` object.\n",
        "*   **Processes:** This is the core model engine. It initializes a Gurobi model and declares all decision variables ($x_{r,s}$, $t_{r',r,p}$, $v_{r,p}$). Its most critical process is the construction of the fundamental market clearing constraint for every region `r` and product `p`:\n",
        "    $s_{r,p} \\geq d_{r,p}$\n",
        "    It does this by building the Gurobi linear expression for total supply, $s_{r,p}$, and total demand, $d_{r,p}$, with perfect fidelity to their definitions:\n",
        "    $s_{r,p} = \\sum_{s} C_{r,s,p} x_{r,s} + \\sum_{r'} t_{r',r,p}$\n",
        "    $d_{r,p} = \\sum_{r',s} B_{r,p,r',s} x_{r',s} + f_{r,p} + e_{r,p} + n_{r,p} - v_{r,p}$\n",
        "*   **Outputs:** An `MRIAModelComponents` TypedDict containing the Gurobi model object, handles to the variables, and handles to the supply and demand expression arrays.\n",
        "*   **Transformation:** It transforms the numerical coefficient data and symbolic variables into a system of algebraic constraints within the solver's memory.\n",
        "*   **Role in Research Pipeline:** This function is the **Assembler**. It builds the fundamental, unconstrained algebraic structure of the economic system described in the paper.\n",
        "\n",
        "**Callable: `calibrate_alpha_parameter`**\n",
        "*   **Inputs:** The `PreprocessedData` object and the `config` dictionary.\n",
        "*   **Processes:** This function implements the calibration procedure described in Section 2.2. It performs a grid search over a set of candidate `alpha` values. For each `alpha`, it formulates and solves a specific optimization problem representing the baseline economy (no shocks, no rationing). The objective is to find the minimum `alpha` for which the model's optimal solution naturally replicates the observed baseline production and trade flows.\n",
        "*   **Outputs:** A single `float` representing the calibrated `alpha` value (1.25 in the paper).\n",
        "*   **Transformation:** It transforms a set of candidate parameters and a replication criterion into a single, validated parameter.\n",
        "*   **Role in Research Pipeline:** This function is the **Calibrator**. It tunes a key behavioral parameter of the model to ensure that its baseline behavior is consistent with the observed data, a critical step for ensuring the validity of its out-of-equilibrium (i.e., post-disaster) simulations.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 4: Core MRIA Algorithm Implementation**\n",
        "\n",
        "**Callable: `run_mria_core_algorithm` (and its helpers)**\n",
        "*   **Inputs:** The `PreprocessedData` object, the calibrated `alpha`, and a `scenario_config` dictionary defining a specific disruption.\n",
        "*   **Processes:** This orchestrator executes the complete three-step optimization algorithm from Table 1 for a single scenario.\n",
        "    1.  **Step 1 (`_execute_step_1`):** It solves the LP with the objective of minimizing total rationing, subject to the scenario's production and trade constraints.\n",
        "        *   **Equation:** `min z₁ = sum_{r,p}(v_{r,p})`\n",
        "    2.  **Step 2 (`_execute_step_2`):** It solves a new LP where the objective is to minimize economic effort, using the minimized rationing from Step 1 as a new, fixed constraint.\n",
        "        *   **Equation:** `min z₂ = sum_{r,s}(x_{r,s}) + α * sum_{r',r,p}(t_{r',r,p})`\n",
        "    3.  **Step 3 (`_execute_step_3`):** It solves a final, separate LP to find the production equivalent of the rationed goods.\n",
        "        *   **Equation:** `min z₃ = sum_{r,s}(x'_{r,s})`\n",
        "    Finally, it calculates the total economic impact using the results from all three steps.\n",
        "    *   **Equation (4):** `c = Σ(s_i - (s_step2 - v_step2)) + Σ(k_step2) + Σ(x'_step3)`\n",
        "*   **Outputs:** An `MRIASolution` TypedDict containing all optimal variables and calculated impacts for the single scenario.\n",
        "*   **Transformation:** It transforms a scenario definition (a shock) into a comprehensive, quantitative description of the post-disaster economic equilibrium.\n",
        "*   **Role in Research Pipeline:** This is the **Simulation Engine**. It is the heart of the entire analysis, executing the paper's central contribution: the multi-step algorithm for assessing the macroeconomic impacts of a disaster.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 5, 6, 7: Analysis Orchestrators**\n",
        "\n",
        "**Callable: `run_sensitivity_analysis`**\n",
        "*   **Inputs:** `PreprocessedData`, `alpha`, `config`.\n",
        "*   **Processes:** This function orchestrates the analysis from Section 2.4.1. It systematically iterates through a grid of `production_extension_factors` and `trade_flexibility_factors`, calling `run_mria_core_algorithm` for each of the 18 combinations under a fixed disruption. It then aggregates the results and calculates the ancillary trade dependency ratio.\n",
        "*   **Outputs:** A `pandas` DataFrame summarizing the 18 runs and a `pandas` Series with the dependency ratios.\n",
        "*   **Role in Research Pipeline:** Implements the **Sensitivity Analysis**, exploring the parameter space of the model to understand the trade-offs between different resilience strategies.\n",
        "\n",
        "**Callable: `run_criticality_analysis`**\n",
        "*   **Inputs:** `PreprocessedData`, `alpha`, `config`.\n",
        "*   **Processes:** This function orchestrates the analysis from Section 2.4.2. It systematically iterates through every active economic sector, applying a fixed 10% disruption to each one individually while holding the rest of the system in a \"flexible\" state. It calls `run_mria_core_algorithm` for each of these ~660 simulations and records the total system-wide rationing. Finally, it calculates the normalized criticality score, normalized output, and Location Quotient.\n",
        "    *   **Equation (5):** `c_{r,s} = R_{r,s} / sum(R_all)`\n",
        "    *   **Equation (6):** `LQ_{r,s} = (x_{r,s} / X_r) / (X_s / X)`\n",
        "*   **Outputs:** A `pandas` DataFrame containing the criticality metrics for every sector.\n",
        "*   **Role in Research Pipeline:** Implements the **Criticality Analysis**, a key innovation of the paper used to identify systemically important sectors based on their irreplaceability.\n",
        "\n",
        "**Callable: `run_incremental_disruption_analysis`**\n",
        "*   **Inputs:** `PreprocessedData`, `alpha`, `config`.\n",
        "*   **Processes:** This function orchestrates the analysis from Section 2.4.3. For two specific sectors (Chemicals and Petroleum) and two system configurations (rigid and flexible), it iterates through a series of escalating disruption magnitudes (1% to 100%). It calls `run_mria_core_algorithm` for each of the 52 simulations and records the total rationing.\n",
        "*   **Outputs:** A multi-indexed `pandas` DataFrame tracing the rationing curve for each experiment.\n",
        "*   **Role in Research Pipeline:** Implements the **Incremental Disruption (or Transition) Analysis**, revealing the non-linear response of the system to increasing stress and identifying critical failure thresholds.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 8 & 9: Master Orchestrators**\n",
        "\n",
        "**Callable: `run_mria_pipeline`**\n",
        "*   **Inputs:** The four raw DataFrames and the `config`.\n",
        "*   **Processes:** This function serves as the primary orchestrator for the main analyses. It executes the entire sequence of validation, preprocessing, calibration, and the conditional execution of the sensitivity, criticality, and incremental disruption analyses.\n",
        "*   **Outputs:** A dictionary containing the results of all executed analyses.\n",
        "*   **Role in Research Pipeline:** This is the **Main Experiment Controller**, providing a single entry point to replicate the core results of the paper.\n",
        "\n",
        "**Callable: `run_robustness_analyses` (and its helpers)**\n",
        "*   **Inputs:** The four raw DataFrames and the `config`.\n",
        "*   **Processes:** This function serves as the orchestrator for the advanced robustness tests. Based on flags in the `config`, it conditionally calls the specialized functions for testing sensitivity to `alpha`, disruption magnitude, structural coefficients (via Monte Carlo), and model formulation. Each of these helpers re-runs parts of the main pipeline under modified conditions and compares the results.\n",
        "*   **Outputs:** A dictionary containing the results of all executed robustness tests.\n",
        "*   **Role in Research Pipeline:** This is the **Meta-Analysis Controller**. It executes experiments *about* the main experiment, testing the stability and reliability of the core findings.\n",
        "\n",
        "**Callable: `run_full_experiment`**\n",
        "*   **Inputs:** The four raw DataFrames and the `config`.\n",
        "*   **Processes:** This is the ultimate, top-level function. It makes exactly two calls: one to `run_mria_pipeline` and one to `run_robustness_analyses`.\n",
        "*   **Outputs:** A final, nested dictionary containing all possible results from the entire project.\n",
        "*   **Role in Research Pipeline:** This is the **Master Entry Point**, providing a single command to run the entire research project from start to finish.\n",
        "\n",
        "## Usage Examples\n",
        "\n",
        "### **Example Usage of the End-to-End MRIA Pipeline**\n",
        "\n",
        "This example demonstrates how a researcher would use the master orchestrator function, `run_full_experiment`, to execute the analyses described in the paper. It covers the setup of input data structures and the configuration dictionary.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 1: Assembling the Input Data**\n",
        "\n",
        "The pipeline requires four `pandas` DataFrames as its primary data inputs. These would typically be loaded from CSV files, a database, or another data source. For this example, we will assume they are loaded into memory and have the precise structure validated by Task 1.\n",
        "\n",
        "**A. `initial_production` DataFrame**\n",
        "\n",
        "This DataFrame contains the baseline economic output for every active sector in every region.\n",
        "\n",
        "*   **Structure:** A `pandas.DataFrame` with a `MultiIndex` of `['region', 'sector']` and a single `float64` column named `'production_value'`.\n",
        "*   **Example Snippet:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# In a real scenario, this would be loaded:\n",
        "# initial_production = pd.read_csv(\"initial_production.csv\", index_col=['region', 'sector'])\n",
        "\n",
        "# For demonstration, let's create a small, valid sample:\n",
        "prod_data = {\n",
        "    ('NL33', 'C19'): 500.0,  # Zuid-Holland, Coke & Petroleum\n",
        "    ('NL33', 'C20'): 350.0,  # Zuid-Holland, Chemicals\n",
        "    ('NL41', 'C20'): 200.0,  # Noord-Brabant, Chemicals\n",
        "    ('NL11', 'A01'): 150.0,  # Groningen, Agriculture\n",
        "    # ... and so on for all 660 region-sector pairs\n",
        "}\n",
        "initial_production = pd.DataFrame.from_dict(\n",
        "    prod_data, orient='index', columns=['production_value']\n",
        ")\n",
        "initial_production.index.names = ['region', 'sector']\n",
        "\n",
        "print(\"--- Sample Initial Production DataFrame ---\")\n",
        "print(initial_production.head())\n",
        "```\n",
        "\n",
        "**B. `initial_trade` DataFrame**\n",
        "\n",
        "This DataFrame contains the baseline value of inter-regional trade for each product.\n",
        "\n",
        "*   **Structure:** A `pandas.DataFrame` with a `MultiIndex` of `['origin_region', 'dest_region', 'product']` and a single `float64` column named `'trade_value'`.\n",
        "*   **Example Snippet:**\n",
        "\n",
        "```python\n",
        "# In a real scenario, this would be loaded:\n",
        "# initial_trade = pd.read_csv(\"initial_trade.csv\", index_col=['origin_region', 'dest_region', 'product'])\n",
        "\n",
        "# For demonstration, a small, valid sample:\n",
        "trade_data = {\n",
        "    ('NL41', 'NL33', 'PROD_CHEM'): 50.0, # N.Brabant -> Z.Holland, Chemical Products\n",
        "    ('NL33', 'NL31', 'PROD_FUEL'): 80.0, # Z.Holland -> Utrecht, Fuel Products\n",
        "    # ... and so on for all trade links\n",
        "}\n",
        "initial_trade = pd.DataFrame.from_dict(\n",
        "    trade_data, orient='index', columns=['trade_value']\n",
        ")\n",
        "initial_trade.index.names = ['origin_region', 'dest_region', 'product']\n",
        "\n",
        "print(\"\\n--- Sample Initial Trade DataFrame ---\")\n",
        "print(initial_trade.head())\n",
        "```\n",
        "\n",
        "**C. `supply_coefficients` and `use_coefficients` DataFrames**\n",
        "\n",
        "These DataFrames contain the technical coefficients that define the economy's production recipes.\n",
        "\n",
        "*   **Structure:** As defined and validated in Task 1.\n",
        "*   **Example Snippet:**\n",
        "\n",
        "```python\n",
        "# These would be loaded from a source like the RHOMOLO V4 dataset.\n",
        "# supply_coefficients = pd.read_csv(\"supply_coeffs.csv\", index_col=['region', 'sector', 'product'])\n",
        "# use_coefficients = pd.read_csv(\"use_coeffs.csv\", index_col=['dest_region', 'dest_sector', 'product', 'origin_region'])\n",
        "\n",
        "# For demonstration, minimal valid samples:\n",
        "supply_data = {\n",
        "    ('NL33', 'C19', 'PROD_FUEL'): 0.95, # 95% of C19 output is Fuel\n",
        "    ('NL33', 'C19', 'PROD_CHEM'): 0.05, # 5% is Chemical by-products\n",
        "    ('NL33', 'C20', 'PROD_CHEM'): 1.00, # 100% of C20 output is Chemicals\n",
        "    # ... ensuring sum over products for each (region, sector) is 1.0\n",
        "}\n",
        "supply_coefficients = pd.DataFrame.from_dict(\n",
        "    supply_data, orient='index', columns=['coefficient']\n",
        ")\n",
        "supply_coefficients.index.names = ['region', 'sector', 'product']\n",
        "\n",
        "use_data = {\n",
        "    ('NL33', 'C20', 'PROD_FUEL', 'NL33'): 0.2, # Z.Holland's Chemical sector uses Fuel from Z.Holland\n",
        "    # ... and so on for all input-output relationships\n",
        "}\n",
        "use_coefficients = pd.DataFrame.from_dict(\n",
        "    use_data, orient='index', columns=['coefficient']\n",
        ")\n",
        "use_coefficients.index.names = ['dest_region', 'dest_sector', 'product', 'origin_region']\n",
        "\n",
        "print(\"\\n--- Sample Supply Coefficients ---\")\n",
        "print(supply_coefficients.head())\n",
        "```\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 2: Defining the Configuration Dictionary**\n",
        "\n",
        "The `config` dictionary is the master control panel for the entire experiment. It allows the user to enable or disable specific analyses and set all relevant parameters without touching the underlying code. For this example, we will construct a complete `config` that includes the new `robustness_analysis` section.\n",
        "\n",
        "*   **Structure:** A nested Python dictionary.\n",
        "*   **Example Snippet:**\n",
        "\n",
        "```python\n",
        "# This is the complete configuration dictionary that controls the entire pipeline.\n",
        "# It includes the original config plus the new section for robustness tests.\n",
        "full_config = {\n",
        "    # A. GENERAL MODEL PARAMETERS\n",
        "    'general_parameters': {\n",
        "        'alpha_trade_cost': 1.25,\n",
        "    },\n",
        "    # B. SENSITIVITY ANALYSIS CONFIGURATION\n",
        "    'sensitivity_analysis': {\n",
        "        'run_analysis': True,\n",
        "        'disruption_scenario': {\n",
        "            'target_region_code': 'NL33',\n",
        "            'target_sectors': ['C19', 'C20'], # A smaller list for a quicker example\n",
        "            'disruption_magnitude': 0.10,\n",
        "        },\n",
        "        'parameter_grid': {\n",
        "            'production_extension_factors': [0.0, 0.025, 0.10],\n",
        "            'trade_flexibility_factors': [0.0, 0.25, 1.00],\n",
        "        },\n",
        "    },\n",
        "    # C. CRITICALITY ANALYSIS CONFIGURATION\n",
        "    'criticality_analysis': {\n",
        "        'run_analysis': True,\n",
        "        'disruption_magnitude': 0.10,\n",
        "        'flexible_system_parameters': {\n",
        "            'production_extension_factor': 0.025,\n",
        "            'trade_flexibility_factor': 1.00,\n",
        "        },\n",
        "    },\n",
        "    # D. INCREMENTAL DISRUPTION ANALYSIS CONFIGURATION\n",
        "    'incremental_disruption_analysis': {\n",
        "        'run_analysis': True,\n",
        "        'target_scenarios': [\n",
        "            {'name': 'Chemicals', 'target_region_code': 'NL33', 'target_sector_code': 'C20'},\n",
        "        ],\n",
        "        'disruption_levels': [0.10, 0.50, 0.90], # A smaller grid for a quicker example\n",
        "        'system_configurations': {\n",
        "            'rigid': {'production_extension_factor': 0.00, 'trade_flexibility_factor': 0.00},\n",
        "            'flexible': {'production_extension_factor': 0.025, 'trade_flexibility_factor': 1.00},\n",
        "        },\n",
        "    },\n",
        "    # E. ROBUSTNESS ANALYSIS SUITE CONFIGURATION\n",
        "    'robustness_analysis': {\n",
        "        'run_alpha_robustness': True,\n",
        "        'alpha_grid': [1.1, 1.25, 1.4],\n",
        "        \n",
        "        'run_disruption_magnitude_robustness': False, # Disabled for this example run\n",
        "        'disruption_grid': [0.05, 0.10, 0.15],\n",
        "        \n",
        "        'run_structural_robustness': False, # Disabled (very computationally expensive)\n",
        "        'structural_robustness_params': {\n",
        "            'num_iterations': 10, 'perturbation_factor': 0.05, 'random_seed': 42\n",
        "        },\n",
        "        \n",
        "        'run_formulation_robustness': True,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n--- Master Configuration Dictionary (Excerpt) ---\")\n",
        "print(f\"Sensitivity Analysis Enabled: {full_config['sensitivity_analysis']['run_analysis']}\")\n",
        "print(f\"Alpha Robustness Enabled: {full_config['robustness_analysis']['run_alpha_robustness']}\")\n",
        "```\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 3: Executing the Pipeline**\n",
        "\n",
        "With the data and configuration prepared, the final step is a single call to the master orchestrator function.\n",
        "\n",
        "*   **Function:** `run_full_experiment`\n",
        "*   **Example Snippet:**\n",
        "\n",
        "```python\n",
        "# This is the final step: a single call to the master orchestrator.\n",
        "# It assumes all required functions and the data/config from above are in scope.\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # It is best practice to run the main experiment inside this block.\n",
        "#\n",
        "#     # Create placeholder data for the example to run\n",
        "#     # In a real run, you would load the full, valid datasets.\n",
        "#     # This part is just to make the example runnable.\n",
        "#     regions = {f'NL{r}' for r in ['11', '31', '33', '41']}\n",
        "#     sectors = {f'C{s}' for s in ['19', '20']} | {'A01'}\n",
        "#     products = {'PROD_FUEL', 'PROD_CHEM'}\n",
        "#     \n",
        "#     # Create dummy data that passes validation\n",
        "#     prod_idx = pd.MultiIndex.from_product([regions, sectors], names=['region', 'sector'])\n",
        "#     initial_production = pd.DataFrame(np.random.rand(len(prod_idx), 1) * 100, index=prod_idx, columns=['production_value'])\n",
        "#\n",
        "#     trade_idx = pd.MultiIndex.from_product([regions, regions, products], names=['origin_region', 'dest_region', 'product'])\n",
        "#     initial_trade = pd.DataFrame(np.random.rand(len(trade_idx), 1) * 10, index=trade_idx, columns=['trade_value'])\n",
        "#     initial_trade = initial_trade[initial_trade.index.get_level_values(0) != initial_trade.index.get_level_values(1)] # Inter-regional only\n",
        "#\n",
        "#     supply_idx = pd.MultiIndex.from_product([regions, sectors, products], names=['region', 'sector', 'product'])\n",
        "#     supply_coefficients = pd.DataFrame(np.random.rand(len(supply_idx), 1), index=supply_idx, columns=['coefficient'])\n",
        "#     supply_coefficients = supply_coefficients.groupby(['region', 'sector']).transform(lambda x: x / x.sum()) # Normalize\n",
        "#\n",
        "#     use_idx = pd.MultiIndex.from_product([regions, sectors, products, regions], names=['dest_region', 'dest_sector', 'product', 'origin_region'])\n",
        "#     use_coefficients = pd.DataFrame(np.random.rand(len(use_idx), 1) * 0.1, index=use_idx, columns=['coefficient'])\n",
        "#\n",
        "#     print(\"\\n--- EXECUTING THE FULL MRIA EXPERIMENT PIPELINE ---\")\n",
        "#     # The single call that runs everything.\n",
        "#     # full_experiment_results = run_full_experiment(\n",
        "#     #     initial_production,\n",
        "#     #     initial_trade,\n",
        "#     #     supply_coefficients,\n",
        "#     #     use_coefficients,\n",
        "#     #     full_config\n",
        "#     # )\n",
        "#\n",
        "#     # print(\"\\n--- PIPELINE EXECUTION COMPLETE ---\")\n",
        "#     #\n",
        "#     # # --- Step 4: Inspecting the Results ---\n",
        "#     # print(\"\\nAvailable result keys:\", full_experiment_results.keys())\n",
        "#     #\n",
        "#     # # Example: Access the results of the criticality analysis\n",
        "#     # crit_results = full_experiment_results['main_analysis_results']['criticality_analysis_results']\n",
        "#     # print(\"\\nTop 5 Most Critical Sectors (from main analysis):\")\n",
        "#     # print(crit_results.sort_values('criticality_score', ascending=False).head())\n",
        "#     #\n",
        "#     # # Example: Access the results of the alpha robustness test\n",
        "#     # alpha_robustness_results = full_experiment_results['robustness_analysis_results']['alpha_robustness_results']\n",
        "#     # print(\"\\nAlpha Robustness Results (Rank Correlation):\")\n",
        "#     # print(alpha_robustness_results[['alpha_value', 'rank_correlation_with_baseline']].drop_duplicates().dropna())\n",
        "\n",
        "print(\"\\nNOTE: The final execution is commented out to prevent running a long computation. \"\n",
        "      \"The code demonstrates the complete setup and the final call to the pipeline.\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "UxZn4ctnvyVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Input Data Validation and Quality Assurance\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Input Data Validation and Quality Assurance\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: DataFrame Structure and Type Validation (Sub-Task 1.1)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_structure(\n",
        "    df: pd.DataFrame,\n",
        "    df_name: str,\n",
        "    expected_index_levels: int,\n",
        "    expected_index_names: List[str],\n",
        "    expected_columns: Dict[str, type],\n",
        "    expected_index_content: Dict[str, Set[str]] = None,\n",
        "    max_rows: int = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the structure, index, columns, and dtypes of a DataFrame.\n",
        "\n",
        "    This is a generic helper function used by the specific validation functions\n",
        "    to ensure that input DataFrames conform to the required schema for the\n",
        "    MRIA model.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to validate.\n",
        "        df_name (str): The name of the DataFrame (for error messages).\n",
        "        expected_index_levels (int): The expected number of levels in the MultiIndex.\n",
        "        expected_index_names (List[str]): The expected names of the index levels.\n",
        "        expected_columns (Dict[str, type]): A dictionary mapping expected column\n",
        "                                            names to their expected numpy dtypes.\n",
        "        expected_index_content (Dict[str, Set[str]], optional): A dictionary\n",
        "            mapping an index level name to a set of required values.\n",
        "            Defaults to None.\n",
        "        max_rows (int, optional): The maximum number of rows the DataFrame\n",
        "                                  can have. Defaults to None.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If any structural validation check fails.\n",
        "    \"\"\"\n",
        "    # Check if the input is a pandas DataFrame\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(f\"Input '{df_name}' must be a pandas DataFrame, but got {type(df)}.\")\n",
        "\n",
        "    # --- Index Validation ---\n",
        "    # Verify the number of index levels\n",
        "    if df.index.nlevels != expected_index_levels:\n",
        "        raise ValueError(\n",
        "            f\"'{df_name}' DataFrame must have a MultiIndex with \"\n",
        "            f\"{expected_index_levels} levels, but found {df.index.nlevels}.\"\n",
        "        )\n",
        "\n",
        "    # Verify the names of the index levels\n",
        "    if list(df.index.names) != expected_index_names:\n",
        "        raise ValueError(\n",
        "            f\"'{df_name}' DataFrame must have index levels named \"\n",
        "            f\"{expected_index_names}, but found {list(df.index.names)}.\"\n",
        "        )\n",
        "\n",
        "    # Verify the uniqueness of the index\n",
        "    if not df.index.is_unique:\n",
        "        raise ValueError(f\"Index of '{df_name}' DataFrame contains duplicate entries.\")\n",
        "\n",
        "    # --- Column and Dtype Validation ---\n",
        "    # Verify the number of columns\n",
        "    if len(df.columns) != len(expected_columns):\n",
        "        raise ValueError(\n",
        "            f\"'{df_name}' DataFrame must have {len(expected_columns)} column(s), \"\n",
        "            f\"but found {len(df.columns)}.\"\n",
        "        )\n",
        "\n",
        "    # Verify column names and dtypes\n",
        "    for col, expected_dtype in expected_columns.items():\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"'{df_name}' DataFrame is missing required column: '{col}'.\")\n",
        "        if df[col].dtype != expected_dtype:\n",
        "            raise TypeError(\n",
        "                f\"Column '{col}' in '{df_name}' DataFrame must have dtype \"\n",
        "                f\"{expected_dtype}, but found {df[col].dtype}.\"\n",
        "            )\n",
        "\n",
        "    # --- Content Validation ---\n",
        "    # Verify the content of specified index levels\n",
        "    if expected_index_content:\n",
        "        for level_name, required_values in expected_index_content.items():\n",
        "            present_values = set(df.index.get_level_values(level_name))\n",
        "            if not present_values.issubset(required_values):\n",
        "                unknown_values = present_values - required_values\n",
        "                raise ValueError(\n",
        "                    f\"Index level '{level_name}' in '{df_name}' contains \"\n",
        "                    f\"unexpected values: {sorted(list(unknown_values))[:5]}...\"\n",
        "                )\n",
        "\n",
        "    # Verify the maximum number of rows\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        raise ValueError(\n",
        "            f\"'{df_name}' DataFrame cannot have more than {max_rows} rows, \"\n",
        "            f\"but found {len(df)}.\"\n",
        "        )\n",
        "\n",
        "    # Verify no missing values in any column\n",
        "    if df.isnull().values.any():\n",
        "        raise ValueError(f\"'{df_name}' DataFrame contains missing (NaN) values.\")\n",
        "\n",
        "\n",
        "def validate_initial_production(\n",
        "    df: pd.DataFrame,\n",
        "    valid_regions: Set[str],\n",
        "    valid_sectors: Set[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs validation for the Initial Production DataFrame (Sub-Task 1.1.1).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The Initial Production DataFrame.\n",
        "        valid_regions (Set[str]): A set of valid NUTS-2 region codes.\n",
        "        valid_sectors (Set[str]): A set of valid NACE sector codes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If data-specific validation fails.\n",
        "    \"\"\"\n",
        "    # Use the generic structure validator\n",
        "    _validate_dataframe_structure(\n",
        "        df=df,\n",
        "        df_name=\"Initial Production\",\n",
        "        expected_index_levels=2,\n",
        "        expected_index_names=['region', 'sector'],\n",
        "        expected_columns={'production_value': np.float64},\n",
        "        expected_index_content={'region': valid_regions, 'sector': valid_sectors},\n",
        "        max_rows=len(valid_regions) * len(valid_sectors)\n",
        "    )\n",
        "\n",
        "    # --- Data-Specific Economic Validation ---\n",
        "    # Verify all production values are non-negative: production_value_r,s >= 0\n",
        "    if (df['production_value'] < 0).any():\n",
        "        raise ValueError(\n",
        "            \"Column 'production_value' in Initial Production DataFrame \"\n",
        "            \"must contain non-negative values.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def validate_initial_trade(\n",
        "    df: pd.DataFrame,\n",
        "    valid_regions: Set[str],\n",
        "    valid_products: Set[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs validation for the Initial Trade DataFrame (Sub-Task 1.1.2).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The Initial Trade DataFrame.\n",
        "        valid_regions (Set[str]): A set of valid NUTS-2 region codes.\n",
        "        valid_products (Set[str]): A set of valid product codes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If data-specific validation fails.\n",
        "    \"\"\"\n",
        "    # Use the generic structure validator\n",
        "    _validate_dataframe_structure(\n",
        "        df=df,\n",
        "        df_name=\"Initial Trade\",\n",
        "        expected_index_levels=3,\n",
        "        expected_index_names=['origin_region', 'dest_region', 'product'],\n",
        "        expected_columns={'trade_value': np.float64},\n",
        "        expected_index_content={\n",
        "            'origin_region': valid_regions,\n",
        "            'dest_region': valid_regions,\n",
        "            'product': valid_products\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # --- Data-Specific Economic and Structural Validation ---\n",
        "    # Verify all trade values are non-negative: trade_value_r',r,p >= 0\n",
        "    if (df['trade_value'] < 0).any():\n",
        "        raise ValueError(\n",
        "            \"Column 'trade_value' in Initial Trade DataFrame must contain \"\n",
        "            \"non-negative values.\"\n",
        "        )\n",
        "\n",
        "    # Verify strict inter-regional trade constraint: origin_region != dest_region\n",
        "    intra_regional_trade = df.index.get_level_values('origin_region') == df.index.get_level_values('dest_region')\n",
        "    if intra_regional_trade.any():\n",
        "        raise ValueError(\n",
        "            \"Initial Trade DataFrame must only contain inter-regional trade \"\n",
        "            \"(origin_region != dest_region), but intra-regional flows were found.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def validate_technical_coefficients(\n",
        "    supply_coeffs: pd.DataFrame,\n",
        "    use_coeffs: pd.DataFrame,\n",
        "    initial_production: pd.DataFrame,\n",
        "    valid_regions: Set[str],\n",
        "    valid_sectors: Set[str],\n",
        "    valid_products: Set[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs validation for Supply and Use Coefficients (Sub-Task 1.1.3).\n",
        "\n",
        "    Args:\n",
        "        supply_coeffs (pd.DataFrame): The Supply Technical Coefficients DataFrame.\n",
        "        use_coeffs (pd.DataFrame): The Use Technical Coefficients DataFrame.\n",
        "        initial_production (pd.DataFrame): The Initial Production DataFrame, used\n",
        "                                           for normalization checks.\n",
        "        valid_regions (Set[str]): A set of valid NUTS-2 region codes.\n",
        "        valid_sectors (Set[str]): A set of valid NACE sector codes.\n",
        "        valid_products (Set[str]): A set of valid product codes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If data-specific validation fails.\n",
        "    \"\"\"\n",
        "    # --- Supply Coefficients Validation ---\n",
        "    _validate_dataframe_structure(\n",
        "        df=supply_coeffs,\n",
        "        df_name=\"Supply Coefficients\",\n",
        "        expected_index_levels=3,\n",
        "        expected_index_names=['region', 'sector', 'product'],\n",
        "        expected_columns={'coefficient': np.float64},\n",
        "        expected_index_content={\n",
        "            'region': valid_regions,\n",
        "            'sector': valid_sectors,\n",
        "            'product': valid_products\n",
        "        }\n",
        "    )\n",
        "    # Verify coefficient bounds: 0 <= coefficient <= 1\n",
        "    if not ((supply_coeffs['coefficient'] >= 0) & (supply_coeffs['coefficient'] <= 1)).all():\n",
        "        raise ValueError(\"Supply coefficients must be between 0 and 1.\")\n",
        "\n",
        "    # --- Use Coefficients Validation ---\n",
        "    _validate_dataframe_structure(\n",
        "        df=use_coeffs,\n",
        "        df_name=\"Use Coefficients\",\n",
        "        expected_index_levels=4,\n",
        "        expected_index_names=['dest_region', 'dest_sector', 'product', 'origin_region'],\n",
        "        expected_columns={'coefficient': np.float64},\n",
        "        expected_index_content={\n",
        "            'dest_region': valid_regions,\n",
        "            'dest_sector': valid_sectors,\n",
        "            'product': valid_products,\n",
        "            'origin_region': valid_regions\n",
        "        }\n",
        "    )\n",
        "    # Verify coefficient bounds: 0 <= coefficient <= 1\n",
        "    if not ((use_coeffs['coefficient'] >= 0) & (use_coeffs['coefficient'] <= 1)).all():\n",
        "        raise ValueError(\"Use coefficients must be between 0 and 1.\")\n",
        "\n",
        "    # --- Supply Coefficient Normalization Check ---\n",
        "    # Equation: sum_p(C_r,s,p) = 1.0 for each (r, s) with positive production\n",
        "    # Identify region-sector pairs with positive production\n",
        "    positive_production_pairs = initial_production[initial_production['production_value'] > 1e-9].index\n",
        "\n",
        "    # Filter coefficients for these pairs and check normalization\n",
        "    if not positive_production_pairs.empty:\n",
        "        relevant_coeffs = supply_coeffs.loc[supply_coeffs.index.droplevel('product').isin(positive_production_pairs)]\n",
        "        # Group by region and sector and sum the coefficients\n",
        "        summed_coeffs = relevant_coeffs.groupby(['region', 'sector'])['coefficient'].sum()\n",
        "\n",
        "        # Check if all sums are close to 1.0\n",
        "        if not np.allclose(summed_coeffs, 1.0, atol=1e-6):\n",
        "            failing_pairs = summed_coeffs[~np.isclose(summed_coeffs, 1.0, atol=1e-6)]\n",
        "            raise ValueError(\n",
        "                \"Supply coefficients for sectors with positive production must sum to 1.0. \"\n",
        "                f\"Found {len(failing_pairs)} failing (region, sector) pairs. \"\n",
        "                f\"Example failure:\\n{failing_pairs.head()}\"\n",
        "            )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Parameter Configuration Validation (Sub-Task 1.2)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_config_schema(config: Dict[str, Any], schema: Dict[str, Any], path: str = \"config\") -> None:\n",
        "    \"\"\"\n",
        "    Recursively validates a nested dictionary against a schema.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "        schema (Dict[str, Any]): The schema to validate against.\n",
        "        path (str): The current path in the dictionary (for error messages).\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If a value has the wrong type.\n",
        "        ValueError: If a key is missing or a value is out of bounds.\n",
        "    \"\"\"\n",
        "    # Check for missing keys required by the schema\n",
        "    if not set(schema.keys()).issubset(set(config.keys())):\n",
        "        missing_keys = set(schema.keys()) - set(config.keys())\n",
        "        raise ValueError(f\"Missing required keys at '{path}': {missing_keys}\")\n",
        "\n",
        "    # Iterate through schema keys to validate config\n",
        "    for key, rules in schema.items():\n",
        "        # Construct the current path for error reporting\n",
        "        current_path = f\"{path}['{key}']\"\n",
        "        value = config[key]\n",
        "\n",
        "        # Validate the type of the value\n",
        "        if not isinstance(value, rules['type']):\n",
        "            raise TypeError(f\"Expected type {rules['type']} for '{current_path}', but got {type(value)}.\")\n",
        "\n",
        "        # Apply value constraints if they exist\n",
        "        if 'constraints' in rules:\n",
        "            for constraint in rules['constraints']:\n",
        "                if not constraint(value):\n",
        "                    raise ValueError(f\"Value for '{current_path}' failed constraint check: {constraint.__doc__}\")\n",
        "\n",
        "        # Recursively validate nested dictionaries\n",
        "        if 'nested' in rules and isinstance(value, dict):\n",
        "            _validate_config_schema(value, rules['nested'], current_path)\n",
        "\n",
        "        # Validate lists of dictionaries\n",
        "        if 'list_of_dicts' in rules and isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if not isinstance(item, dict):\n",
        "                    raise TypeError(f\"Expected list of dicts for '{current_path}', but item {i} is {type(item)}.\")\n",
        "                _validate_config_schema(item, rules['list_of_dicts'], f\"{current_path}[{i}]\")\n",
        "\n",
        "\n",
        "def validate_config(config: Dict[str, Any], valid_regions: Set[str], valid_sectors: Set[str]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the master configuration dictionary (Sub-Task 1.2).\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "        valid_regions (Set[str]): A set of valid NUTS-2 region codes.\n",
        "        valid_sectors (Set[str]): A set of valid NACE sector codes.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If config is not a dictionary.\n",
        "        ValueError: If any validation check fails.\n",
        "    \"\"\"\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(f\"Configuration must be a dictionary, but got {type(config)}.\")\n",
        "\n",
        "    # Define the validation schema\n",
        "    schema = {\n",
        "        'general_parameters': {\n",
        "            'type': dict,\n",
        "            'nested': {\n",
        "                'alpha_trade_cost': {\n",
        "                    'type': float,\n",
        "                    'constraints': [lambda v: 1.0 <= v <= 2.0]\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'sensitivity_analysis': {\n",
        "            'type': dict,\n",
        "            'nested': {\n",
        "                'run_analysis': {'type': bool},\n",
        "                'disruption_scenario': {\n",
        "                    'type': dict,\n",
        "                    'nested': {\n",
        "                        'target_region_code': {\n",
        "                            'type': str,\n",
        "                            'constraints': [lambda v: v in valid_regions]\n",
        "                        },\n",
        "                        'target_sectors': {\n",
        "                            'type': list,\n",
        "                            'constraints': [lambda v: all(s in valid_sectors for s in v)]\n",
        "                        },\n",
        "                        'disruption_magnitude': {\n",
        "                            'type': float,\n",
        "                            'constraints': [lambda v: 0.0 < v <= 1.0]\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                'parameter_grid': {\n",
        "                    'type': dict,\n",
        "                    'nested': {\n",
        "                        'production_extension_factors': {'type': list},\n",
        "                        'trade_flexibility_factors': {'type': list}\n",
        "                    }\n",
        "                },\n",
        "                'output_figure_path': {'type': str}\n",
        "            }\n",
        "        },\n",
        "        'criticality_analysis': {\n",
        "            'type': dict,\n",
        "            'nested': {\n",
        "                'run_analysis': {'type': bool},\n",
        "                'disruption_magnitude': {\n",
        "                    'type': float,\n",
        "                    'constraints': [lambda v: 0.0 < v <= 1.0]\n",
        "                },\n",
        "                'flexible_system_parameters': {\n",
        "                    'type': dict,\n",
        "                    'nested': {\n",
        "                        'production_extension_factor': {'type': float},\n",
        "                        'trade_flexibility_factor': {'type': float}\n",
        "                    }\n",
        "                },\n",
        "                'output_figure_path_prefix': {'type': str}\n",
        "            }\n",
        "        },\n",
        "        'incremental_disruption_analysis': {\n",
        "            'type': dict,\n",
        "            'nested': {\n",
        "                'run_analysis': {'type': bool},\n",
        "                'target_scenarios': {\n",
        "                    'type': list,\n",
        "                    'list_of_dicts': {\n",
        "                        'name': {'type': str},\n",
        "                        'target_region_code': {\n",
        "                            'type': str,\n",
        "                            'constraints': [lambda v: v in valid_regions]\n",
        "                        },\n",
        "                        'target_sector_code': {\n",
        "                            'type': str,\n",
        "                            'constraints': [lambda v: v in valid_sectors]\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                'disruption_levels': {\n",
        "                    'type': list,\n",
        "                    'constraints': [lambda v: all(0.0 < x <= 1.0 for x in v) and v == sorted(v)]\n",
        "                },\n",
        "                'system_configurations': {'type': dict},\n",
        "                'output_figure_path': {'type': str}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Run the validation\n",
        "    _validate_config_schema(config, schema)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Cross-DataFrame and Economic Balance Validation (Sub-Task 1.3)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_economic_balance(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs economic balance validation checks (Sub-Task 1.3.3).\n",
        "\n",
        "    Specifically, it verifies the export feasibility constraint:\n",
        "    Total exports from a region cannot exceed its total production.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): The Initial Production DataFrame.\n",
        "        initial_trade (pd.DataFrame): The Initial Trade DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the export feasibility constraint is violated.\n",
        "    \"\"\"\n",
        "    # Equation: Export_r <= X_r for all regions r\n",
        "    # Calculate total regional production: X_r = sum_s(production_value_r,s)\n",
        "    regional_production = initial_production.groupby('region')['production_value'].sum()\n",
        "\n",
        "    # Calculate total regional exports: Export_r = sum_r'!=r,p(trade_value_r,r',p)\n",
        "    regional_exports = initial_trade.groupby('origin_region')['trade_value'].sum()\n",
        "\n",
        "    # Align the two series for comparison, filling missing export regions with 0\n",
        "    regional_exports = regional_exports.reindex(regional_production.index, fill_value=0)\n",
        "\n",
        "    # Check if exports exceed production for any region\n",
        "    infeasible_exports = regional_exports > regional_production + 1e-9 # Add tolerance\n",
        "    if infeasible_exports.any():\n",
        "        failing_regions = regional_production[infeasible_exports].to_dict()\n",
        "        export_values = regional_exports[infeasible_exports].to_dict()\n",
        "        raise ValueError(\n",
        "            \"Export feasibility constraint violated. Total exports exceed total \"\n",
        "            f\"production for the following regions: {list(failing_regions.keys())}.\\n\"\n",
        "            f\"Production: {failing_regions}\\n\"\n",
        "            f\"Exports: {export_values}\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1 Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_input_validation_and_quality_assurance(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[Set[str], Set[str], Set[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of all input data and configurations.\n",
        "\n",
        "    This function serves as the single entry point for Task 1, executing all\n",
        "    sub-tasks in a logical sequence. It validates data structures, economic\n",
        "    constraints, parameter configurations, and cross-data consistency.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production values.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade flows.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply technical coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use technical coefficients.\n",
        "        config (Dict[str, Any]): The master configuration dictionary for the analyses.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Set[str], Set[str], Set[str]]: A tuple containing the validated sets of\n",
        "        unique region codes, sector codes, and product codes, which can be used\n",
        "        in subsequent data preprocessing steps.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If any input is of an incorrect type.\n",
        "        ValueError: If any validation check fails.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Task 1: Input Data Validation and Quality Assurance ---\")\n",
        "\n",
        "    # --- Extract authoritative sets of identifiers ---\n",
        "    # These sets define the universe of regions, sectors, and products.\n",
        "    valid_regions = set(initial_production.index.get_level_values('region'))\n",
        "    valid_sectors = set(initial_production.index.get_level_values('sector'))\n",
        "    valid_products = set(initial_trade.index.get_level_values('product'))\n",
        "    print(f\"Found {len(valid_regions)} regions, {len(valid_sectors)} sectors, and {len(valid_products)} products.\")\n",
        "\n",
        "    # --- Step 1.1: DataFrame Structure and Type Validation ---\n",
        "    print(\"Step 1.1: Validating DataFrame structures and types...\")\n",
        "    validate_initial_production(initial_production, valid_regions, valid_sectors)\n",
        "    validate_initial_trade(initial_trade, valid_regions, valid_products)\n",
        "    validate_technical_coefficients(\n",
        "        supply_coeffs=supply_coefficients,\n",
        "        use_coeffs=use_coefficients,\n",
        "        initial_production=initial_production,\n",
        "        valid_regions=valid_regions,\n",
        "        valid_sectors=valid_sectors,\n",
        "        valid_products=valid_products\n",
        "    )\n",
        "    print(\"...DataFrame structures are valid.\")\n",
        "\n",
        "    # --- Step 1.2: Parameter Configuration Validation ---\n",
        "    print(\"Step 1.2: Validating parameter configuration dictionary...\")\n",
        "    validate_config(config, valid_regions, valid_sectors)\n",
        "    print(\"...Configuration dictionary is valid.\")\n",
        "\n",
        "    # --- Step 1.3: Economic Balance and Consistency Validation ---\n",
        "    print(\"Step 1.3: Validating economic balance constraints...\")\n",
        "    validate_economic_balance(initial_production, initial_trade)\n",
        "    # Note: Cross-DataFrame consistency of codes is implicitly handled by\n",
        "    # passing the authoritative sets to the validation functions in Step 1.1.\n",
        "    print(\"...Economic balance constraints are satisfied.\")\n",
        "\n",
        "    print(\"--- Task 1 Successfully Completed: All inputs are valid. ---\")\n",
        "\n",
        "    # Return the authoritative sets for use in the next task\n",
        "    return valid_regions, valid_sectors, valid_products\n"
      ],
      "metadata": {
        "id": "2Aexxmjqvzvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing and Matrix Construction\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Data Preprocessing and Matrix Construction\n",
        "# ==============================================================================\n",
        "\n",
        "# Define a TypedDict for the structured output of the preprocessing step.\n",
        "# This enhances code readability and allows for static type checking.\n",
        "class PreprocessedData(TypedDict):\n",
        "    \"\"\"\n",
        "    A dictionary holding all preprocessed data structures for the MRIA model.\n",
        "\n",
        "    This structure ensures that all necessary components of the baseline economy\n",
        "    are consistently indexed and readily available for the optimization steps.\n",
        "    \"\"\"\n",
        "    # Index Mappings\n",
        "    region_to_idx: Dict[str, int]\n",
        "    idx_to_region: Dict[int, str]\n",
        "    sector_to_idx: Dict[str, int]\n",
        "    idx_to_sector: Dict[int, str]\n",
        "    product_to_idx: Dict[str, int]\n",
        "    idx_to_product: Dict[int, str]\n",
        "\n",
        "    # Dimensions\n",
        "    num_regions: int\n",
        "    num_sectors: int\n",
        "    num_products: int\n",
        "\n",
        "    # Baseline Economic Variables (as numpy arrays)\n",
        "    baseline_production: np.ndarray  # Shape: (num_regions, num_sectors)\n",
        "    baseline_trade: np.ndarray       # Shape: (num_regions, num_regions, num_products)\n",
        "\n",
        "    # Final Demand Components (as numpy arrays)\n",
        "    # NOTE: These are assumed to be derivable from a more complete dataset.\n",
        "    # Here we initialize them as zeros, as per the model's baseline setup.\n",
        "    final_consumption: np.ndarray    # Shape: (num_regions, num_products)\n",
        "    exports_row: np.ndarray          # Shape: (num_regions, num_products)\n",
        "    reconstruction_demand: np.ndarray # Shape: (num_regions, num_products)\n",
        "\n",
        "    # Technical Coefficient Tensors (as numpy arrays)\n",
        "    supply_coeffs: np.ndarray        # Shape: (num_regions, num_sectors, num_products)\n",
        "    use_coeffs: np.ndarray           # Shape: (num_dest_regions, num_dest_sectors, num_products, num_origin_regions)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Index Mapping and Baseline Matrix Construction (Sub-Task 2.1)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_index_mappings(\n",
        "    regions: Set[str],\n",
        "    sectors: Set[str],\n",
        "    products: Set[str]\n",
        ") -> Tuple[Dict[str, int], ...]:\n",
        "    \"\"\"\n",
        "    Creates deterministic, sorted index mappings for regions, sectors, and products.\n",
        "\n",
        "    Args:\n",
        "        regions (Set[str]): A set of unique region codes.\n",
        "        sectors (Set[str]): A set of unique sector codes.\n",
        "        products (Set[str]): A set of unique product codes.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, int], ...]: A tuple containing six dictionaries:\n",
        "        (region_to_idx, idx_to_region, sector_to_idx, idx_to_sector,\n",
        "         product_to_idx, idx_to_product).\n",
        "    \"\"\"\n",
        "    # Sort the codes alphabetically to ensure a deterministic order\n",
        "    sorted_regions = sorted(list(regions))\n",
        "    sorted_sectors = sorted(list(sectors))\n",
        "    sorted_products = sorted(list(products))\n",
        "\n",
        "    # Create forward (code -> index) and reverse (index -> code) mappings\n",
        "    region_to_idx = {code: i for i, code in enumerate(sorted_regions)}\n",
        "    idx_to_region = {i: code for i, code in enumerate(sorted_regions)}\n",
        "    sector_to_idx = {code: i for i, code in enumerate(sorted_sectors)}\n",
        "    idx_to_sector = {i: code for i, code in enumerate(sorted_sectors)}\n",
        "    product_to_idx = {code: i for i, code in enumerate(sorted_products)}\n",
        "    idx_to_product = {i: code for i, code in enumerate(sorted_products)}\n",
        "\n",
        "    return (\n",
        "        region_to_idx, idx_to_region,\n",
        "        sector_to_idx, idx_to_sector,\n",
        "        product_to_idx, idx_to_product\n",
        "    )\n",
        "\n",
        "\n",
        "def construct_baseline_production_matrix(\n",
        "    initial_production: pd.DataFrame,\n",
        "    region_map: Dict[str, int],\n",
        "    sector_map: Dict[str, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the baseline production matrix (x_bar_r,s).\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): Validated initial production data.\n",
        "        region_map (Dict[str, int]): Mapping from region codes to integer indices.\n",
        "        sector_map (Dict[str, int]): Mapping from sector codes to integer indices.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D numpy array of shape (num_regions, num_sectors)\n",
        "                    representing baseline production.\n",
        "    \"\"\"\n",
        "    # Get the canonical ordering from the maps\n",
        "    ordered_regions = list(region_map.keys())\n",
        "    ordered_sectors = list(sector_map.keys())\n",
        "\n",
        "    # Reindex the DataFrame to match the canonical order and fill missing values\n",
        "    # This ensures the unstacked matrix has the correct dimensions and order.\n",
        "    multi_index = pd.MultiIndex.from_product(\n",
        "        [ordered_regions, ordered_sectors],\n",
        "        names=['region', 'sector']\n",
        "    )\n",
        "    production_reindexed = initial_production.reindex(multi_index, fill_value=0.0)\n",
        "\n",
        "    # Unstack the 'sector' level to create the matrix format\n",
        "    production_matrix = production_reindexed.unstack(level='sector')\n",
        "\n",
        "    # Extract the values as a numpy array\n",
        "    # The column names will be ('production_value', sector_code), so we access\n",
        "    # the underlying numpy array directly.\n",
        "    return production_matrix.values\n",
        "\n",
        "\n",
        "def construct_baseline_trade_tensor(\n",
        "    initial_trade: pd.DataFrame,\n",
        "    region_map: Dict[str, int],\n",
        "    product_map: Dict[str, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the baseline inter-regional trade tensor (t_bar_r',r,p).\n",
        "\n",
        "    Args:\n",
        "        initial_trade (pd.DataFrame): Validated initial trade data.\n",
        "        region_map (Dict[str, int]): Mapping from region codes to integer indices.\n",
        "        product_map (Dict[str, int]): Mapping from product codes to integer indices.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 3D numpy array of shape (origin_regions, dest_regions,\n",
        "                    num_products) representing baseline trade flows.\n",
        "    \"\"\"\n",
        "    # Get dimensions and canonical ordering\n",
        "    num_regions = len(region_map)\n",
        "    num_products = len(product_map)\n",
        "    ordered_regions = list(region_map.keys())\n",
        "    ordered_products = list(product_map.keys())\n",
        "\n",
        "    # Create a complete index scaffold to ensure all non-traded pairs are zero\n",
        "    full_index = pd.MultiIndex.from_product(\n",
        "        [ordered_regions, ordered_regions, ordered_products],\n",
        "        names=['origin_region', 'dest_region', 'product']\n",
        "    )\n",
        "\n",
        "    # Reindex the trade data against the full scaffold\n",
        "    trade_reindexed = initial_trade.reindex(full_index, fill_value=0.0)\n",
        "\n",
        "    # Reshape the series into the desired 3D tensor\n",
        "    # The .values accessor returns a 1D array, which we reshape\n",
        "    trade_tensor = trade_reindexed['trade_value'].values.reshape(\n",
        "        num_regions, num_regions, num_products\n",
        "    )\n",
        "\n",
        "    return trade_tensor\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Technical Coefficient Tensor Construction (Sub-Task 2.2)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_supply_coefficient_tensor(\n",
        "    supply_coeffs_df: pd.DataFrame,\n",
        "    region_map: Dict[str, int],\n",
        "    sector_map: Dict[str, int],\n",
        "    product_map: Dict[str, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the supply coefficient tensor (C_r,s,p).\n",
        "\n",
        "    Args:\n",
        "        supply_coeffs_df (pd.DataFrame): Validated supply coefficients data.\n",
        "        region_map (Dict[str, int]): Mapping from region codes to integer indices.\n",
        "        sector_map (Dict[str, int]): Mapping from sector codes to integer indices.\n",
        "        product_map (Dict[str, int]): Mapping from product codes to integer indices.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 3D numpy array of shape (num_regions, num_sectors,\n",
        "                    num_products).\n",
        "    \"\"\"\n",
        "    # Get dimensions\n",
        "    num_regions = len(region_map)\n",
        "    num_sectors = len(sector_map)\n",
        "    num_products = len(product_map)\n",
        "\n",
        "    # Initialize a zero tensor with the correct shape\n",
        "    C_tensor = np.zeros((num_regions, num_sectors, num_products), dtype=np.float64)\n",
        "\n",
        "    # Get integer indices for each row in the DataFrame for efficient assignment\n",
        "    region_indices = supply_coeffs_df.index.get_level_values('region').map(region_map)\n",
        "    sector_indices = supply_coeffs_df.index.get_level_values('sector').map(sector_map)\n",
        "    product_indices = supply_coeffs_df.index.get_level_values('product').map(product_map)\n",
        "\n",
        "    # Use advanced numpy indexing to populate the tensor in a single, vectorized operation\n",
        "    C_tensor[region_indices, sector_indices, product_indices] = supply_coeffs_df['coefficient'].values\n",
        "\n",
        "    return C_tensor\n",
        "\n",
        "\n",
        "def construct_use_coefficient_tensor(\n",
        "    use_coeffs_df: pd.DataFrame,\n",
        "    region_map: Dict[str, int],\n",
        "    sector_map: Dict[str, int],\n",
        "    product_map: Dict[str, int]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the use coefficient tensor (B_r,p,r',s).\n",
        "\n",
        "    The tensor shape is (dest_region, dest_sector, product, origin_region) to\n",
        "    align with the structure of the input DataFrame and facilitate constraint\n",
        "    construction.\n",
        "\n",
        "    Args:\n",
        "        use_coeffs_df (pd.DataFrame): Validated use coefficients data.\n",
        "        region_map (Dict[str, int]): Mapping from region codes to integer indices.\n",
        "        sector_map (Dict[str, int]): Mapping from sector codes to integer indices.\n",
        "        product_map (Dict[str, int]): Mapping from product codes to integer indices.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 4D numpy array of shape (num_dest_regions,\n",
        "                    num_dest_sectors, num_products, num_origin_regions).\n",
        "    \"\"\"\n",
        "    # Get dimensions\n",
        "    num_regions = len(region_map)\n",
        "    num_sectors = len(sector_map)\n",
        "    num_products = len(product_map)\n",
        "\n",
        "    # Initialize a zero tensor with the correct 4D shape\n",
        "    B_tensor = np.zeros(\n",
        "        (num_regions, num_sectors, num_products, num_regions),\n",
        "        dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # Get integer indices for each dimension from the DataFrame index\n",
        "    dest_region_indices = use_coeffs_df.index.get_level_values('dest_region').map(region_map)\n",
        "    dest_sector_indices = use_coeffs_df.index.get_level_values('dest_sector').map(sector_map)\n",
        "    product_indices = use_coeffs_df.index.get_level_values('product').map(product_map)\n",
        "    origin_region_indices = use_coeffs_df.index.get_level_values('origin_region').map(region_map)\n",
        "\n",
        "    # Use advanced numpy indexing for efficient, vectorized population\n",
        "    B_tensor[\n",
        "        dest_region_indices,\n",
        "        dest_sector_indices,\n",
        "        product_indices,\n",
        "        origin_region_indices\n",
        "    ] = use_coeffs_df['coefficient'].values\n",
        "\n",
        "    return B_tensor\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2 Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def preprocess_data_for_mria(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    validated_sets: Tuple[Set[str], Set[str], Set[str]]\n",
        ") -> PreprocessedData:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete data preprocessing pipeline for the MRIA model.\n",
        "\n",
        "    This function transforms validated pandas DataFrames into a structured\n",
        "    dictionary of numpy arrays and index mappings, which serve as the direct\n",
        "    input for the optimization model. It ensures all data is consistently\n",
        "    ordered and shaped according to the model's mathematical formulation.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): Validated baseline production data.\n",
        "        initial_trade (pd.DataFrame): Validated baseline inter-regional trade data.\n",
        "        supply_coefficients (pd.DataFrame): Validated supply coefficients data.\n",
        "        use_coefficients (pd.DataFrame): Validated use coefficients data.\n",
        "        validated_sets (Tuple[Set[str], Set[str], Set[str]]): A tuple containing\n",
        "            the validated sets of regions, sectors, and products from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        PreprocessedData: A TypedDict containing all necessary numpy arrays and\n",
        "                          mappings for the MRIA model.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Task 2: Data Preprocessing and Matrix Construction ---\")\n",
        "\n",
        "    # Unpack the validated sets of identifiers\n",
        "    valid_regions, valid_sectors, valid_products = validated_sets\n",
        "\n",
        "    # --- Step 2.1: Create canonical index mappings ---\n",
        "    print(\"Step 2.1: Creating canonical index mappings...\")\n",
        "    (\n",
        "        region_to_idx, idx_to_region,\n",
        "        sector_to_idx, idx_to_sector,\n",
        "        product_to_idx, idx_to_product\n",
        "    ) = _create_index_mappings(valid_regions, valid_sectors, valid_products)\n",
        "\n",
        "    num_regions = len(valid_regions)\n",
        "    num_sectors = len(valid_sectors)\n",
        "    num_products = len(valid_products)\n",
        "    print(f\"...Mappings created for {num_regions} regions, {num_sectors} sectors, {num_products} products.\")\n",
        "\n",
        "    # --- Step 2.1: Construct baseline economic variable matrices ---\n",
        "    print(\"Step 2.1: Constructing baseline production and trade matrices...\")\n",
        "    baseline_production_matrix = construct_baseline_production_matrix(\n",
        "        initial_production, region_to_idx, sector_to_idx\n",
        "    )\n",
        "    baseline_trade_tensor = construct_baseline_trade_tensor(\n",
        "        initial_trade, region_to_idx, product_to_idx\n",
        "    )\n",
        "    print(\"...Baseline matrices constructed.\")\n",
        "\n",
        "    # --- Step 2.1.2: Initialize final demand components ---\n",
        "    # In a full implementation, these would be derived from more detailed SUTs.\n",
        "    # For this model, they are initialized to zero for the baseline scenario.\n",
        "    print(\"Step 2.1: Initializing final demand components...\")\n",
        "    final_consumption = np.zeros((num_regions, num_products), dtype=np.float64)\n",
        "    exports_row = np.zeros((num_regions, num_products), dtype=np.float64)\n",
        "    reconstruction_demand = np.zeros((num_regions, num_products), dtype=np.float64)\n",
        "    print(\"...Final demand components initialized to zero.\")\n",
        "\n",
        "    # --- Step 2.2: Construct technical coefficient tensors ---\n",
        "    print(\"Step 2.2: Constructing technical coefficient tensors...\")\n",
        "    supply_coeffs_tensor = construct_supply_coefficient_tensor(\n",
        "        supply_coefficients, region_to_idx, sector_to_idx, product_to_idx\n",
        "    )\n",
        "    use_coeffs_tensor = construct_use_coefficient_tensor(\n",
        "        use_coefficients, region_to_idx, sector_to_idx, product_to_idx\n",
        "    )\n",
        "    print(\"...Coefficient tensors constructed.\")\n",
        "\n",
        "    # --- Assemble the final preprocessed data object ---\n",
        "    preprocessed_data: PreprocessedData = {\n",
        "        'region_to_idx': region_to_idx,\n",
        "        'idx_to_region': idx_to_region,\n",
        "        'sector_to_idx': sector_to_idx,\n",
        "        'idx_to_sector': idx_to_sector,\n",
        "        'product_to_idx': product_to_idx,\n",
        "        'idx_to_product': idx_to_product,\n",
        "        'num_regions': num_regions,\n",
        "        'num_sectors': num_sectors,\n",
        "        'num_products': num_products,\n",
        "        'baseline_production': baseline_production_matrix,\n",
        "        'baseline_trade': baseline_trade_tensor,\n",
        "        'final_consumption': final_consumption,\n",
        "        'exports_row': exports_row,\n",
        "        'reconstruction_demand': reconstruction_demand,\n",
        "        'supply_coeffs': supply_coeffs_tensor,\n",
        "        'use_coeffs': use_coeffs_tensor\n",
        "    }\n",
        "\n",
        "    print(\"--- Task 2 Successfully Completed: Data preprocessed into numpy arrays. ---\")\n",
        "\n",
        "    return preprocessed_data\n"
      ],
      "metadata": {
        "id": "Z2tkYDtNv5Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Model Calibration Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Model Calibration Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "# Define a TypedDict for the structured output of the model builder.\n",
        "class MRIAModelComponents(TypedDict):\n",
        "    \"\"\"\n",
        "    A dictionary holding the Gurobi model, its core variable objects, and\n",
        "    the key algebraic expressions for supply and demand.\n",
        "    \"\"\"\n",
        "    model: gp.Model\n",
        "    x: gp.MVar                  # Production variables (x_r,s)\n",
        "    t: gp.MVar                  # Trade variables (t_r',r,p)\n",
        "    v: gp.MVar                  # Rationing variables (v_r,p)\n",
        "    s: np.ndarray               # Array of Gurobi LinExpr for supply (s_r,p)\n",
        "    d: np.ndarray               # Array of Gurobi LinExpr for demand (d_r,p)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1 & 2: Core LP Model Construction & Solver Configuration (Sub-Tasks 3.2, 3.3)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _build_mria_lp_model(\n",
        "    data: 'PreprocessedData'\n",
        ") -> MRIAModelComponents:\n",
        "    \"\"\"\n",
        "    Builds the core Gurobi LP model for the MRIA framework (REMEDIATED).\n",
        "\n",
        "    This function constructs the decision variables and the fundamental constraints\n",
        "    (supply-demand balance) of the MRIA model using compliant and accurate\n",
        "    Gurobi expression-building techniques. It replaces the flawed numpy-based\n",
        "    operations on Gurobi variables with a robust, loop-based construction\n",
        "    that directly mirrors the model's mathematical formulation.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The dictionary of preprocessed numpy arrays and\n",
        "                                 mappings from Task 2.\n",
        "\n",
        "    Returns:\n",
        "        MRIAModelComponents: A TypedDict containing the Gurobi model object,\n",
        "                             references to the decision variable MVars, and\n",
        "                             numpy arrays of the supply and demand LinExpr objects.\n",
        "    \"\"\"\n",
        "    # --- Initialize Gurobi Model ---\n",
        "    # Create a new Gurobi model environment with suppressed output for clean execution.\n",
        "    env = gp.Env(empty=True)\n",
        "    env.setParam('OutputFlag', 0)\n",
        "    env.start()\n",
        "    model = gp.Model(\"MRIA_Remediated\", env=env)\n",
        "\n",
        "    # --- Set High-Precision Solver Parameters ---\n",
        "    # These settings are crucial for numerical stability in complex economic models.\n",
        "    model.setParam(GRB.Param.FeasibilityTol, 1e-9)\n",
        "    model.setParam(GRB.Param.OptimalityTol, 1e-9)\n",
        "    model.setParam(GRB.Param.NumericFocus, 3)\n",
        "\n",
        "    # --- Unpack Dimensions for Clarity ---\n",
        "    R, S, P = data['num_regions'], data['num_sectors'], data['num_products']\n",
        "\n",
        "    # --- Declare Decision Variables ---\n",
        "    # Production variables x_r,s >= 0\n",
        "    x = model.addMVar((R, S), vtype=GRB.CONTINUOUS, name=\"x\", lb=0.0)\n",
        "\n",
        "    # Disaster trade variables t_r',r,p >= 0 (origin_r, dest_r, product_p)\n",
        "    t = model.addMVar((R, R, P), vtype=GRB.CONTINUOUS, name=\"t\", lb=0.0)\n",
        "\n",
        "    # Rationing variables v_r,p >= 0\n",
        "    v = model.addMVar((R, P), vtype=GRB.CONTINUOUS, name=\"v\", lb=0.0)\n",
        "\n",
        "    # --- Unpack Data Tensors for Building Expressions ---\n",
        "    C = data['supply_coeffs']\n",
        "    B = data['use_coeffs']\n",
        "    f_bar = data['final_consumption']\n",
        "    e_bar = data['exports_row']\n",
        "    n_bar = data['reconstruction_demand']\n",
        "\n",
        "    # --- Construct Core Gurobi Expressions (The Remediation) ---\n",
        "    # Initialize numpy arrays to hold the Gurobi Linear Expression objects.\n",
        "    s_expr = np.empty((R, P), dtype=gp.LinExpr)\n",
        "    d_expr = np.empty((R, P), dtype=gp.LinExpr)\n",
        "\n",
        "    # Iterate over each region (r) and product (p) to build expressions.\n",
        "    for r_idx in range(R):\n",
        "        for p_idx in range(P):\n",
        "            # --- 1. Construct Total Supply Expression (s_r,p) ---\n",
        "            # Equation: s_r,p = sum_s(C_r,s,p * x_r,s) + sum_r'(t_r',r,p)\n",
        "\n",
        "            # Production component: sum over sectors 's' in region 'r'.\n",
        "            supply_from_production = gp.quicksum(\n",
        "                C[r_idx, s_idx, p_idx] * x[r_idx, s_idx] for s_idx in range(S)\n",
        "            )\n",
        "\n",
        "            # Trade component: sum over origin regions 'r'' for a given destination 'r'.\n",
        "            supply_from_trade = t.sum(axis=0)[r_idx, p_idx]\n",
        "\n",
        "            # Store the complete supply expression.\n",
        "            s_expr[r_idx, p_idx] = supply_from_production + supply_from_trade\n",
        "\n",
        "            # --- 2. Construct Total Demand Expression (d_r,p) ---\n",
        "            # Equation: d_r,p = sum_r',s(B_r,p,r',s * x_r',s) + f_r,p + e_r,p + n_r,p - v_r,p\n",
        "\n",
        "            # Intermediate demand component: sum over all origin regions 'r'' and all sectors 's'.\n",
        "            # B tensor shape: (dest_r, dest_s, prod, orig_r)\n",
        "            # x variable shape: (orig_r, orig_s)\n",
        "            # For a given demand at (dest_r=r_idx, prod=p_idx), we sum over all possible\n",
        "            # production locations (orig_r, orig_s) that supply inputs.\n",
        "            intermediate_demand = gp.quicksum(\n",
        "                B[r_idx, s_dest_idx, p_idx, r_orig_idx] * x[r_orig_idx, s_orig_idx]\n",
        "                for r_orig_idx in range(R)\n",
        "                for s_orig_idx in range(S)\n",
        "                for s_dest_idx in range(S) # Summing over all using sectors in the destination region\n",
        "            )\n",
        "\n",
        "            # Combine with final demand components and rationing.\n",
        "            d_expr[r_idx, p_idx] = (\n",
        "                intermediate_demand +\n",
        "                f_bar[r_idx, p_idx] +\n",
        "                e_bar[r_idx, p_idx] +\n",
        "                n_bar[r_idx, p_idx] -\n",
        "                v[r_idx, p_idx]\n",
        "            )\n",
        "\n",
        "    # --- Add the Market Clearing Constraint to the model ---\n",
        "    # Equation: s_r,p >= d_r,p for all r, p\n",
        "    model.addConstr(s_expr >= d_expr, name=\"supply_demand_balance\")\n",
        "\n",
        "    # --- Package and Return Model Components ---\n",
        "    # The returned components now include the Gurobi expressions, which is a key improvement.\n",
        "    model_components: MRIAModelComponents = {\n",
        "        \"model\": model,\n",
        "        \"x\": x,\n",
        "        \"t\": t,\n",
        "        \"v\": v,\n",
        "        \"s\": s_expr,\n",
        "        \"d\": d_expr\n",
        "    }\n",
        "    return model_components\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Alpha Selection Algorithm Implementation (Sub-Task 3.1)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def calibrate_alpha_parameter(\n",
        "    data: 'PreprocessedData',\n",
        "    config: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calibrates the alpha trade cost parameter (REMEDIATED).\n",
        "\n",
        "    This function performs a grid search to find the minimum alpha value that\n",
        "    ensures the model, under baseline (no-disruption) conditions, naturally\n",
        "    replicates the initial economic state (production and trade) within a\n",
        "    tight tolerance. This corrected version tests the model's inherent\n",
        "    equilibrium properties rather than forcing the solution.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        config (Dict[str, Any]): The master configuration dictionary, which may\n",
        "                                 contain a list of alpha candidates.\n",
        "\n",
        "    Returns:\n",
        "        float: The calibrated value of alpha.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the optimization fails for any alpha candidate, or if\n",
        "                      no suitable alpha is found in the provided grid.\n",
        "    \"\"\"\n",
        "    # Announce the start of the task.\n",
        "    print(\"--- Starting Task 3 (Remediation): Model Calibration ---\")\n",
        "    print(\"Calibrating alpha parameter for trade cost...\")\n",
        "\n",
        "    # --- Step 1: Define Calibration Scenario and Criteria ---\n",
        "    # Define the grid of alpha candidates to test. Use a default if not in config.\n",
        "    alpha_candidates = config.get('calibration', {}).get('alpha_grid',\n",
        "        [1.0, 1.1, 1.2, 1.25, 1.3, 1.4, 1.5, 2.0]\n",
        "    )\n",
        "    # Ensure candidates are sorted to find the minimum value first.\n",
        "    alpha_candidates.sort()\n",
        "\n",
        "    # Define the strict convergence tolerances for replication.\n",
        "    PRODUCTION_TOLERANCE = 1e-6\n",
        "    TRADE_TOLERANCE = 1e-6\n",
        "\n",
        "    # Unpack baseline data for comparison.\n",
        "    x_bar = data['baseline_production']\n",
        "    t_bar = data['baseline_trade']\n",
        "\n",
        "    # --- Step 2: Iterate Through Alpha Candidates (Grid Search) ---\n",
        "    for alpha in alpha_candidates:\n",
        "        # Provide feedback on the current step of the grid search.\n",
        "        print(f\"  Testing alpha = {alpha:.2f}...\")\n",
        "\n",
        "        # --- Step 2a: Build the core model for the baseline scenario ---\n",
        "        # This uses the corrected model builder from the previous step.\n",
        "        components = _build_mria_lp_model(data)\n",
        "        model, x, t, v = (\n",
        "            components['model'], components['x'], components['t'], components['v']\n",
        "        )\n",
        "\n",
        "        # --- Step 2b: Set the Step 2 objective function for this calibration run ---\n",
        "        # Equation: min z2 = sum_r,s(x_r,s) + alpha * sum_r',r,p(t_r',r,p)\n",
        "        model.setObjective(x.sum() + alpha * t.sum(), GRB.MINIMIZE)\n",
        "\n",
        "        # --- Step 2c: Set Baseline Constraints (No Disruption) ---\n",
        "        # In the baseline scenario, there is no rationing.\n",
        "        # Constraint: v_r,p = 0 for all r, p\n",
        "        model.addConstr(v == 0, name=\"no_rationing\")\n",
        "\n",
        "        # Set generous, non-binding upper bounds for production and trade to\n",
        "        # simulate an unconstrained system and test its natural equilibrium.\n",
        "        # This is a robust way to prevent solver issues with unboundedness.\n",
        "        generous_prod_bound = np.sum(x_bar) * 10\n",
        "        generous_trade_bound = np.sum(t_bar) * 10\n",
        "        model.addConstr(x <= generous_prod_bound, name=\"generous_prod_ub\")\n",
        "        model.addConstr(t <= generous_trade_bound, name=\"generous_trade_ub\")\n",
        "\n",
        "        # --- Step 3a: Solve the Optimization Problem ---\n",
        "        model.optimize()\n",
        "\n",
        "        # --- Step 3b & 3c: Check for Optimality and Verify Replication ---\n",
        "        if model.status == GRB.OPTIMAL:\n",
        "            # Extract the production and trade solutions.\n",
        "            x_solution = x.X\n",
        "            t_solution = t.X\n",
        "\n",
        "            # Check 1: Does the optimal production replicate the baseline?\n",
        "            production_replicated = np.allclose(\n",
        "                x_solution, x_bar, atol=PRODUCTION_TOLERANCE\n",
        "            )\n",
        "\n",
        "            # Check 2: Is there any additional trade beyond the baseline?\n",
        "            # The difference should be less than the tolerance.\n",
        "            additional_trade = (t_solution - t_bar).max()\n",
        "            no_additional_trade = additional_trade < TRADE_TOLERANCE\n",
        "\n",
        "            # Provide detailed feedback on the checks.\n",
        "            max_prod_abs_dev = np.max(np.abs(x_solution - x_bar))\n",
        "            print(f\"    Max absolute production deviation: {max_prod_abs_dev:.2e}\")\n",
        "            print(f\"    Max additional trade: {additional_trade:.2e}\")\n",
        "\n",
        "            # --- Step 3d: Select Alpha if Criteria are Met ---\n",
        "            if production_replicated and no_additional_trade:\n",
        "                print(\"--- Calibration Successful ---\")\n",
        "                print(f\"Selected alpha = {alpha} successfully replicates the baseline economy.\")\n",
        "                # Dispose of the model to free up Gurobi resources.\n",
        "                model.dispose()\n",
        "                # Return the first value that works, which is the minimum.\n",
        "                return alpha\n",
        "        else:\n",
        "            # Handle non-optimal solutions gracefully.\n",
        "            model.dispose()\n",
        "            raise RuntimeError(\n",
        "                f\"Gurobi optimization failed during calibration for alpha = {alpha} \"\n",
        "                f\"with status code {model.status}.\"\n",
        "            )\n",
        "\n",
        "        # Dispose of the model environment before the next iteration.\n",
        "        model.dispose()\n",
        "\n",
        "    # --- Handle Failure Case ---\n",
        "    # If the loop completes without finding a suitable alpha, raise an error.\n",
        "    raise RuntimeError(\n",
        "        \"Calibration failed. No alpha value in the candidate grid \"\n",
        "        f\"{alpha_candidates} could replicate the baseline economy within the \"\n",
        "        f\"specified tolerances (prod_tol={PRODUCTION_TOLERANCE}, \"\n",
        "        f\"trade_tol={TRADE_TOLERANCE}).\"\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "hD_RnnG5xEYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Core MRIA Algorithm Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Core MRIA Algorithm Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "class MRIASolution(TypedDict):\n",
        "    \"\"\"\n",
        "    A dictionary holding the complete solution from the 3-step MRIA algorithm.\n",
        "    \"\"\"\n",
        "    # Scenario Parameters\n",
        "    delta_production_capacity: np.ndarray\n",
        "    phi_trade_flexibility: np.ndarray\n",
        "\n",
        "    # Step 1 Results\n",
        "    min_rationing_vector: np.ndarray # v_r,p\n",
        "\n",
        "    # Step 2 Results\n",
        "    optimal_production: np.ndarray # x_r,s\n",
        "    optimal_trade: np.ndarray      # t_r',r,p\n",
        "    optimal_rationing: np.ndarray  # v_r,p (should match min_rationing_vector)\n",
        "    inefficient_production: np.ndarray # k_r,p\n",
        "\n",
        "    # Step 3 Results\n",
        "    rationing_production_equivalent: float # sum(x'_r,s)\n",
        "\n",
        "    # Economic Impact Calculation Results\n",
        "    total_economic_impact: float\n",
        "    impact_from_supply_reduction: float\n",
        "    impact_from_inefficiency: float\n",
        "    impact_from_rationing: float\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Scenario Parameter Generation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_scenario_parameters(\n",
        "    data: 'PreprocessedData',\n",
        "    scenario_config: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates the delta and phi parameter matrices for a specific scenario.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        scenario_config (Dict[str, Any]): A dictionary defining the scenario,\n",
        "            e.g., {'disruption': {'region': 'NL33', 'sectors': ['C19'], 'magnitude': 0.1},\n",
        "                   'extension_factor': 0.025, 'flexibility_factor': 1.0}.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: A tuple containing the delta matrix\n",
        "        (production capacity) and the phi tensor (trade flexibility).\n",
        "    \"\"\"\n",
        "    # --- Initialize Delta Matrix (Production Capacity) ---\n",
        "    # Start with the economy-wide production extension factor.\n",
        "    # delta_r,s = 1.0 + extension_factor\n",
        "    extension_factor = scenario_config.get('extension_factor', 0.0)\n",
        "    delta = np.full(\n",
        "        (data['num_regions'], data['num_sectors']),\n",
        "        1.0 + extension_factor,\n",
        "        dtype=np.float64\n",
        "    )\n",
        "\n",
        "    # --- Apply Specific Disruptions ---\n",
        "    if 'disruption' in scenario_config:\n",
        "        disruption = scenario_config['disruption']\n",
        "        # Get integer indices for the target region(s) and sector(s)\n",
        "        region_idx = data['region_to_idx'][disruption['region']]\n",
        "        sector_indices = [data['sector_to_idx'][s] for s in disruption['sectors']]\n",
        "\n",
        "        # Apply the disruption magnitude\n",
        "        # delta_r_target,s_target = 1.0 - disruption_magnitude\n",
        "        delta[region_idx, sector_indices] = 1.0 - disruption['magnitude']\n",
        "\n",
        "    # --- Initialize Phi Tensor (Trade Flexibility) ---\n",
        "    # Assume a uniform trade flexibility factor across all trade links.\n",
        "    # phi_r',r,p = flexibility_factor\n",
        "    flexibility_factor = scenario_config.get('flexibility_factor', 0.0)\n",
        "    phi = np.full(\n",
        "        (data['num_regions'], data['num_regions'], data['num_products']),\n",
        "        flexibility_factor,\n",
        "        dtype=np.float64\n",
        "    )\n",
        "\n",
        "    return delta, phi\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Three-Step Optimization Implementation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_step_1(\n",
        "    data: 'PreprocessedData',\n",
        "    delta: np.ndarray,\n",
        "    phi: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Executes Step 1 of the MRIA algorithm: Rationing Minimization.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        delta (np.ndarray): The production capacity matrix for the scenario.\n",
        "        phi (np.ndarray): The trade flexibility tensor for the scenario.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The vector of minimized rationing (v_r,p).\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the optimization does not find an optimal solution.\n",
        "    \"\"\"\n",
        "    # Build the core LP model\n",
        "    components = _build_mria_lp_model(data)\n",
        "    model, x, t, v = components['model'], components['x'], components['t'], components['v']\n",
        "\n",
        "    # --- Add Scenario-Specific Constraints ---\n",
        "    # 1. Production capacity constraint: x_r,s <= x_bar_r,s * delta_r,s\n",
        "    model.addConstr(x <= data['baseline_production'] * delta, name=\"production_capacity\")\n",
        "\n",
        "    # 2. Trade flexibility constraint: t_r',r,p <= t_bar_r',r,p * (1 + phi_r',r,p)\n",
        "    trade_upper_bound = data['baseline_trade'] * (1 + phi)\n",
        "    model.addConstr(t <= trade_upper_bound, name=\"trade_flexibility\")\n",
        "\n",
        "    # 3. Rationing upper bound: v_r,p <= f_bar_r,p + e_bar_r,p + n_bar_r,p\n",
        "    max_rationing = data['final_consumption'] + data['exports_row'] + data['reconstruction_demand']\n",
        "    model.addConstr(v <= max_rationing, name=\"max_rationing\")\n",
        "\n",
        "    # --- Set Step 1 Objective Function ---\n",
        "    # Equation: min z1 = sum_r,p(v_r,p)\n",
        "    model.setObjective(v.sum(), GRB.MINIMIZE)\n",
        "\n",
        "    # --- Solve and Return Results ---\n",
        "    model.optimize()\n",
        "    if model.status != GRB.OPTIMAL:\n",
        "        model.dispose()\n",
        "        raise RuntimeError(f\"MRIA Step 1 failed to find an optimal solution. Status: {model.status}\")\n",
        "\n",
        "    min_rationing_vector = v.X\n",
        "    model.dispose()\n",
        "    return min_rationing_vector\n",
        "\n",
        "\n",
        "def _evaluate_gurobi_expressions(\n",
        "    expressions: np.ndarray,\n",
        "    variables: Dict[str, gp.MVar]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluates a numpy array of Gurobi linear expressions using optimal values.\n",
        "\n",
        "    This helper function is crucial for accurately calculating post-solve metrics\n",
        "    like inefficiency, ensuring the calculation uses the same algebraic\n",
        "    formulation as the solver.\n",
        "\n",
        "    Args:\n",
        "        expressions (np.ndarray): A numpy array where each element is a\n",
        "                                  gurobipy.LinExpr object.\n",
        "        variables (Dict[str, gp.MVar]): A dictionary of the model's MVars\n",
        "                                        ('x', 't', 'v') to access their .X values.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A numpy array of the same shape as the input, containing\n",
        "                    the evaluated numerical values of the expressions.\n",
        "    \"\"\"\n",
        "    # Create an empty numpy array to store the numerical results.\n",
        "    evaluated_values = np.zeros(expressions.shape)\n",
        "\n",
        "    # Use np.nditer for efficient multi-dimensional iteration.\n",
        "    it = np.nditer(expressions, flags=['multi_index', 'refs_ok'])\n",
        "    for expr_obj in it:\n",
        "        # Extract the LinExpr object from the numpy array element.\n",
        "        expr = expr_obj.item()\n",
        "        # Use the Gurobi .getValue() method, which correctly evaluates the\n",
        "        # expression based on the current solution stored in the variables.\n",
        "        evaluated_values[it.multi_index] = expr.getValue()\n",
        "\n",
        "    return evaluated_values\n",
        "\n",
        "\n",
        "def _execute_step_2(\n",
        "    data: 'PreprocessedData',\n",
        "    delta: np.ndarray,\n",
        "    phi: np.ndarray,\n",
        "    min_rationing_vector: np.ndarray,\n",
        "    alpha: float\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Executes Step 2 of the MRIA algorithm: Cost-Optimal Allocation (REMEDIATED).\n",
        "\n",
        "    This function solves for the least-cost production and trade configuration\n",
        "    given the minimum unavoidable rationing determined in Step 1. This remediated\n",
        "    version uses the corrected model builder and implements a robust method for\n",
        "    calculating economic inefficiency post-optimization.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        delta (np.ndarray): The production capacity matrix for the scenario.\n",
        "        phi (np.ndarray): The trade flexibility tensor for the scenario.\n",
        "        min_rationing_vector (np.ndarray): The optimal rationing vector from Step 1.\n",
        "        alpha (float): The calibrated trade cost parameter.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary containing the optimal production,\n",
        "                               trade, rationing, and inefficiency numpy arrays.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the optimization does not find an optimal solution.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Build the core LP model using the corrected builder ---\n",
        "    # This provides the model, variables, and crucial expression objects.\n",
        "    components = _build_mria_lp_model(data)\n",
        "    model, x, t, v, s_expr, d_expr = (\n",
        "        components['model'], components['x'], components['t'], components['v'],\n",
        "        components['s'], components['d']\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Add Scenario-Specific Constraints ---\n",
        "    # Constraint on production capacity.\n",
        "    # Equation: x_r,s <= x_bar_r,s * delta_r,s\n",
        "    model.addConstr(x <= data['baseline_production'] * delta, name=\"production_capacity\")\n",
        "\n",
        "    # Constraint on trade flexibility.\n",
        "    # Equation: t_r',r,p <= t_bar_r',r,p * (1 + phi_r',r,p)\n",
        "    trade_upper_bound = data['baseline_trade'] * (1 + phi)\n",
        "    model.addConstr(t <= trade_upper_bound, name=\"trade_flexibility\")\n",
        "\n",
        "    # CRITICAL STEP: Constrain rationing to the minimum level found in Step 1.\n",
        "    # This links the two optimization steps. A small tolerance is added for\n",
        "    # numerical stability, preventing potential infeasibilities from floating point noise.\n",
        "    # Constraint: v_r,p <= v_min_r,p\n",
        "    model.addConstr(v <= min_rationing_vector + 1e-9, name=\"fixed_rationing_upper_bound\")\n",
        "\n",
        "    # --- Step 3: Set the Step 2 Objective Function ---\n",
        "    # The objective is to minimize the total cost of production and disaster trade.\n",
        "    # Equation: min z2 = sum_r,s(x_r,s) + alpha * sum_r',r,p(t_r',r,p)\n",
        "    model.setObjective(x.sum() + alpha * t.sum(), GRB.MINIMIZE)\n",
        "\n",
        "    # --- Step 4: Solve the Optimization Problem ---\n",
        "    model.optimize()\n",
        "\n",
        "    # Verify that the solver found an optimal solution.\n",
        "    if model.status != GRB.OPTIMAL:\n",
        "        model.dispose()\n",
        "        raise RuntimeError(f\"MRIA Step 2 failed to find an optimal solution. Status: {model.status}\")\n",
        "\n",
        "    # --- Step 5: Process Results and Calculate Inefficiency (The Remediation) ---\n",
        "    # Extract the optimal values of the decision variables.\n",
        "    x_sol, t_sol, v_sol = x.X, t.X, v.X\n",
        "\n",
        "    # Inefficiency is the slack in the market clearing constraint.\n",
        "    # Equation: k_r,p = s_r,p - d_r,p\n",
        "    # We now calculate this by evaluating the Gurobi expressions with the optimal solution.\n",
        "\n",
        "    # Evaluate the supply expression array using the solution values.\n",
        "    s_optimal_values = _evaluate_gurobi_expressions(s_expr, {'x': x, 't': t, 'v': v})\n",
        "\n",
        "    # Evaluate the demand expression array using the solution values.\n",
        "    d_optimal_values = _evaluate_gurobi_expressions(d_expr, {'x': x, 't': t, 'v': v})\n",
        "\n",
        "    # Calculate the inefficiency vector.\n",
        "    inefficient_production = s_optimal_values - d_optimal_values\n",
        "\n",
        "    # Due to floating-point arithmetic, some values might be slightly negative.\n",
        "    # Enforce non-negativity, as inefficiency cannot be negative.\n",
        "    inefficient_production[inefficient_production < 1e-9] = 0.0\n",
        "\n",
        "    # --- Step 6: Package and Return Results ---\n",
        "    # Store all relevant outputs in a structured dictionary.\n",
        "    results = {\n",
        "        \"optimal_production\": x_sol,\n",
        "        \"optimal_trade\": t_sol,\n",
        "        \"optimal_rationing\": v_sol,\n",
        "        \"inefficient_production\": inefficient_production\n",
        "    }\n",
        "\n",
        "    # Dispose of the model to free up Gurobi resources.\n",
        "    model.dispose()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def _execute_step_3(\n",
        "    data: 'PreprocessedData',\n",
        "    min_rationing_vector: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Executes Step 3: Calculates the production equivalent of rationing.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        min_rationing_vector (np.ndarray): The optimal rationing from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        float: The total production equivalent (sum of x'_r,s).\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the optimization does not find an optimal solution.\n",
        "    \"\"\"\n",
        "    # This is a separate, simpler LP.\n",
        "    env = gp.Env(empty=True)\n",
        "    env.setParam('OutputFlag', 0)\n",
        "    env.start()\n",
        "    model = gp.Model(\"MRIA_Step3\", env=env)\n",
        "    model.setParam(GRB.Param.FeasibilityTol, 1e-9)\n",
        "\n",
        "    # --- Variables ---\n",
        "    x_prime = model.addMVar(\n",
        "        (data['num_regions'], data['num_sectors']),\n",
        "        vtype=GRB.CONTINUOUS, name=\"x_prime\", lb=0.0\n",
        "    )\n",
        "\n",
        "    # --- Constraints ---\n",
        "    # s'_r,p >= d'_r,p\n",
        "    # s'_r,p = sum_s(C_r,s,p * x'_r,s)\n",
        "    # d'_r,p = sum_r',s(B_r,p,r',s * x'_r',s) + v_bar_r,p\n",
        "    C, B = data['supply_coeffs'], data['use_coeffs']\n",
        "    s_prime = gp.MVar.fromarray(np.einsum('rsp,rs->rp', C, x_prime.tolist()))\n",
        "    d_prime = gp.MVar.fromarray(np.einsum('dspo,os->dp', B, x_prime.tolist())) + min_rationing_vector\n",
        "    model.addConstr(s_prime >= d_prime, name=\"production_for_rationing\")\n",
        "\n",
        "    # --- Objective ---\n",
        "    # Equation: min z3 = sum_r,s(x'_r,s)\n",
        "    model.setObjective(x_prime.sum(), GRB.MINIMIZE)\n",
        "\n",
        "    # --- Solve and Return ---\n",
        "    model.optimize()\n",
        "    if model.status != GRB.OPTIMAL:\n",
        "        model.dispose()\n",
        "        raise RuntimeError(f\"MRIA Step 3 failed to find an optimal solution. Status: {model.status}\")\n",
        "\n",
        "    total_production_equivalent = model.ObjVal\n",
        "    model.dispose()\n",
        "    return total_production_equivalent\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Economic Impact Calculation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_economic_impacts(\n",
        "    data: 'PreprocessedData',\n",
        "    step2_results: Dict[str, np.ndarray],\n",
        "    rationing_prod_equivalent: float\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculates the total economic impact and its components using Equation (4).\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        step2_results (Dict[str, np.ndarray]): The results from Step 2.\n",
        "        rationing_prod_equivalent (float): The total production from Step 3.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary of the total impact and its components.\n",
        "    \"\"\"\n",
        "    # --- Unpack necessary data ---\n",
        "    x_bar = data['baseline_production']\n",
        "    C = data['supply_coeffs']\n",
        "    s_optimal_step2 = np.einsum('rsp,rs->rp', C, step2_results['optimal_production'])\n",
        "    v_optimal_step2 = step2_results['optimal_rationing']\n",
        "    k_optimal_step2 = step2_results['inefficient_production']\n",
        "\n",
        "    # --- Calculate Baseline Supply (s^i_r,p) ---\n",
        "    s_baseline = np.einsum('rsp,rs->rp', C, x_bar)\n",
        "\n",
        "    # --- Implement Equation (4) ---\n",
        "    # c = sum(s_i - (s_step2 - v_step2)) + sum(k_step2) + sum(x'_step3)\n",
        "\n",
        "    # Component 1: Value of reduced efficient supply\n",
        "    efficient_supply_post_disaster = s_optimal_step2 - v_optimal_step2\n",
        "    impact_supply_reduction = np.sum(s_baseline - efficient_supply_post_disaster)\n",
        "\n",
        "    # Component 2: Value of inefficiently produced products\n",
        "    impact_inefficiency = np.sum(k_optimal_step2)\n",
        "\n",
        "    # Component 3: Production equivalent value of rationed products\n",
        "    impact_rationing = rationing_prod_equivalent\n",
        "\n",
        "    # Total Impact\n",
        "    total_impact = impact_supply_reduction + impact_inefficiency + impact_rationing\n",
        "\n",
        "    return {\n",
        "        \"total_economic_impact\": total_impact,\n",
        "        \"impact_from_supply_reduction\": impact_supply_reduction,\n",
        "        \"impact_from_inefficiency\": impact_inefficiency,\n",
        "        \"impact_from_rationing\": impact_rationing\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4 Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_mria_core_algorithm(\n",
        "    data: 'PreprocessedData',\n",
        "    alpha: float,\n",
        "    scenario_config: Dict[str, Any]\n",
        ") -> MRIASolution:\n",
        "    \"\"\"\n",
        "    Orchestrates the full 3-step MRIA optimization for a single scenario.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary from Task 2.\n",
        "        alpha (float): The calibrated trade cost parameter from Task 3.\n",
        "        scenario_config (Dict[str, Any]): Configuration for the specific\n",
        "                                           disaster scenario to be run.\n",
        "\n",
        "    Returns:\n",
        "        MRIASolution: A TypedDict containing the comprehensive results of the\n",
        "                      simulation.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running MRIA Core Algorithm for scenario: {scenario_config.get('name', 'Custom')} ---\")\n",
        "\n",
        "    # Step 1: Generate scenario-specific parameter matrices\n",
        "    delta, phi = _generate_scenario_parameters(data, scenario_config)\n",
        "    print(\"  Generated scenario parameters (delta and phi).\")\n",
        "\n",
        "    # Step 2: Execute the three-step optimization sequence\n",
        "    print(\"  Executing Step 1: Minimizing Rationing...\")\n",
        "    min_rationing_vector = _execute_step_1(data, delta, phi)\n",
        "    print(f\"    ...Step 1 complete. Min total rationing: {np.sum(min_rationing_vector):.4f}\")\n",
        "\n",
        "    print(\"  Executing Step 2: Minimizing Production and Trade Costs...\")\n",
        "    step2_results = _execute_step_2(data, delta, phi, min_rationing_vector, alpha)\n",
        "    print(\"    ...Step 2 complete.\")\n",
        "\n",
        "    print(\"  Executing Step 3: Calculating Rationing Production Equivalent...\")\n",
        "    rationing_prod_equivalent = _execute_step_3(data, min_rationing_vector)\n",
        "    print(f\"    ...Step 3 complete. Production equivalent: {rationing_prod_equivalent:.4f}\")\n",
        "\n",
        "    # Step 3: Calculate final economic impacts\n",
        "    print(\"  Calculating final economic impacts...\")\n",
        "    impacts = _calculate_economic_impacts(data, step2_results, rationing_prod_equivalent)\n",
        "    print(f\"    ...Total Economic Impact: {impacts['total_economic_impact']:.4f}\")\n",
        "\n",
        "    # Step 4: Assemble the complete solution object\n",
        "    solution: MRIASolution = {\n",
        "        \"delta_production_capacity\": delta,\n",
        "        \"phi_trade_flexibility\": phi,\n",
        "        \"min_rationing_vector\": min_rationing_vector,\n",
        "        \"optimal_production\": step2_results['optimal_production'],\n",
        "        \"optimal_trade\": step2_results['optimal_trade'],\n",
        "        \"optimal_rationing\": step2_results['optimal_rationing'],\n",
        "        \"inefficient_production\": step2_results['inefficient_production'],\n",
        "        \"rationing_production_equivalent\": rationing_prod_equivalent,\n",
        "        **impacts\n",
        "    }\n",
        "\n",
        "    print(\"--- MRIA Core Algorithm execution finished successfully. ---\")\n",
        "    return solution\n"
      ],
      "metadata": {
        "id": "9viN43m7x5iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Sensitivity Analysis Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Sensitivity Analysis Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Result Aggregation and Metric Calculation (Sub-Task 5.3)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_sensitivity_results(\n",
        "    results_list: List[Dict[str, Any]],\n",
        "    data: 'PreprocessedData'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates raw results from multiple MRIA runs into a structured DataFrame.\n",
        "\n",
        "    This function post-processes the detailed solution objects from the sensitivity\n",
        "    analysis, calculating the key scalar metrics required for plotting and\n",
        "    interpretation, as seen in Figure 1 of the paper.\n",
        "\n",
        "    Args:\n",
        "        results_list (List[Dict[str, Any]]): A list where each item is a\n",
        "            dictionary containing the scenario parameters and its MRIASolution.\n",
        "        data (PreprocessedData): The preprocessed data dictionary, needed for\n",
        "                                 baseline values.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where each row corresponds to a scenario run,\n",
        "                      and columns represent the key performance indicators.\n",
        "    \"\"\"\n",
        "    # This list will store the processed data for each scenario run.\n",
        "    processed_records = []\n",
        "\n",
        "    # Calculate the total baseline production once for efficiency.\n",
        "    total_baseline_production = np.sum(data['baseline_production'])\n",
        "\n",
        "    # Iterate through the list of detailed results from each scenario run.\n",
        "    for result in results_list:\n",
        "        # Extract the full solution object for the current scenario.\n",
        "        solution: 'MRIASolution' = result['solution']\n",
        "\n",
        "        # --- Calculate Aggregate Metrics ---\n",
        "        # Total value of rationed products across all regions and products.\n",
        "        total_rationed_value = np.sum(solution['optimal_rationing'])\n",
        "\n",
        "        # Change in total output between pre- and post-disaster conditions.\n",
        "        total_post_disaster_production = np.sum(solution['optimal_production'])\n",
        "        output_change = total_post_disaster_production - total_baseline_production\n",
        "\n",
        "        # Total value of inefficiently produced by-products not used by the economy.\n",
        "        total_inefficiency = np.sum(solution['inefficient_production'])\n",
        "\n",
        "        # Efficient production is total output minus the wasteful component.\n",
        "        efficient_production = total_post_disaster_production - total_inefficiency\n",
        "\n",
        "        # Total value of disaster-related trade flows (t variables).\n",
        "        disaster_trade = np.sum(solution['optimal_trade'])\n",
        "\n",
        "        # The total economic impact as calculated by the comprehensive formula (Eq. 4).\n",
        "        disaster_impact = solution['total_economic_impact']\n",
        "\n",
        "        # --- Store the Processed Record ---\n",
        "        # Create a dictionary representing one row in the final DataFrame.\n",
        "        record = {\n",
        "            'production_extension_factor': result['params']['extension_factor'],\n",
        "            'trade_flexibility_factor': result['params']['flexibility_factor'],\n",
        "            'total_rationed_value': total_rationed_value,\n",
        "            'output_change': output_change,\n",
        "            'total_inefficiency': total_inefficiency,\n",
        "            'efficient_production': efficient_production,\n",
        "            'disaster_trade': disaster_trade,\n",
        "            'disaster_impact': disaster_impact,\n",
        "        }\n",
        "        processed_records.append(record)\n",
        "\n",
        "    # Convert the list of records into a pandas DataFrame.\n",
        "    results_df = pd.DataFrame.from_records(processed_records)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Trade Dependency Analysis (Sub-Task 5.3.3)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_trade_dependency_ratio(\n",
        "    data: 'PreprocessedData',\n",
        "    target_region: str,\n",
        "    target_sectors: List[str]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates trade dependency ratios for all regions relative to a target (REMEDIATED).\n",
        "\n",
        "    This function computes the ratio of a region's consumption of products from\n",
        "    the target sectors to its supply of products to those same sectors. This\n",
        "    remediated version uses the baseline trade tensor directly and filters by\n",
        "    products relevant to the target sectors, providing a more accurate and direct\n",
        "    measure consistent with the paper's supplementary material.\n",
        "\n",
        "    Ratio > 1: Import-oriented (consumes more from target than it supplies to it).\n",
        "    Ratio < 1: Export-oriented (supplies more to target than it consumes from it).\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary.\n",
        "        target_region (str): The code of the region being disrupted.\n",
        "        target_sectors (List[str]): The list of sector codes being disrupted.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series with region codes as the index and their\n",
        "                   calculated dependency ratios as values.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Get Integer Indices from Mappings ---\n",
        "    # Retrieve the integer index for the target region.\n",
        "    target_region_idx = data['region_to_idx'][target_region]\n",
        "    # Retrieve the integer indices for the list of target sectors.\n",
        "    target_sector_indices = [data['sector_to_idx'][s] for s in target_sectors]\n",
        "    # Unpack the baseline trade tensor for easier access.\n",
        "    t_bar = data['baseline_trade']\n",
        "\n",
        "    # --- Step 2: Identify Products Associated with Target Sectors ---\n",
        "    # The dependency is with the *sectors*, so we must consider the products they produce.\n",
        "    # We use the supply coefficient tensor C[r,s,p] for this mapping.\n",
        "    C = data['supply_coeffs']\n",
        "    # A product is considered relevant if any of the target sectors in the target region\n",
        "    # have a non-trivial supply coefficient for it.\n",
        "    relevant_product_mask = C[target_region_idx, target_sector_indices, :].sum(axis=0) > 0.01\n",
        "    relevant_product_indices = np.where(relevant_product_mask)[0]\n",
        "\n",
        "    if relevant_product_indices.size == 0:\n",
        "        # Handle the edge case where target sectors produce no significant products.\n",
        "        print(f\"Warning: Target sectors in {target_region} produce no significant products. Returning empty dependency series.\")\n",
        "        return pd.Series(dtype=np.float64)\n",
        "\n",
        "    # --- Step 3: Calculate Numerator (\"Consumption FROM Target\") ---\n",
        "    # This is the total value of relevant products that each region imports from the target region.\n",
        "    # We slice the trade tensor: t_bar[origin=target, dest=all, product=relevant]\n",
        "    trade_from_target = t_bar[target_region_idx, :, :][:, relevant_product_indices]\n",
        "    # Sum over the product axis to get a vector of total consumption per destination region.\n",
        "    consumption_from_target = trade_from_target.sum(axis=1)\n",
        "\n",
        "    # --- Step 4: Calculate Denominator (\"Supply TO Target\") ---\n",
        "    # This is the total value of relevant products that each region exports to the target region.\n",
        "    # We slice the trade tensor: t_bar[origin=all, dest=target, product=relevant]\n",
        "    trade_to_target = t_bar[:, target_region_idx, :][:, relevant_product_indices]\n",
        "    # Sum over the product axis to get a vector of total supply per origin region.\n",
        "    supply_to_target = trade_to_target.sum(axis=1)\n",
        "\n",
        "    # --- Step 5: Compute the Ratio ---\n",
        "    # Equation: Ratio = Consumption FROM Target / Supply TO Target\n",
        "    # Add a small epsilon to the denominator to prevent division-by-zero errors.\n",
        "    epsilon = 1e-9\n",
        "    ratio = consumption_from_target / (supply_to_target + epsilon)\n",
        "\n",
        "    # --- Step 6: Format the Output ---\n",
        "    # Convert the resulting numpy array to a named pandas Series for clarity.\n",
        "    # The index of the array corresponds to the destination/origin region index.\n",
        "    ratio_series = pd.Series(ratio, index=data['idx_to_region'].values())\n",
        "    ratio_series.name = \"trade_dependency_ratio\"\n",
        "\n",
        "    # The ratio for the target region itself is not meaningful (it's intra-regional), so we drop it.\n",
        "    if target_region in ratio_series.index:\n",
        "        ratio_series = ratio_series.drop(target_region)\n",
        "\n",
        "    return ratio_series\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5 Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_sensitivity_analysis(\n",
        "    data: 'PreprocessedData',\n",
        "    alpha: float,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full sensitivity analysis as described in Section 2.4.1.\n",
        "\n",
        "    This function systematically runs the MRIA model for a grid of parameters\n",
        "    (production extension and trade flexibility) under a fixed disruption\n",
        "    scenario. It then aggregates the results and performs ancillary trade\n",
        "    dependency analysis.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary from Task 2.\n",
        "        alpha (float): The calibrated trade cost parameter from Task 3.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]:\n",
        "        - A DataFrame containing the aggregated results of the 18 scenario runs.\n",
        "        - A Series containing the trade dependency ratios of other regions\n",
        "          relative to the disrupted region and sectors.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Task 5: Sensitivity Analysis ---\")\n",
        "\n",
        "    # Extract the relevant section of the configuration.\n",
        "    sa_config = config['sensitivity_analysis']\n",
        "    if not sa_config['run_analysis']:\n",
        "        print(\"Sensitivity analysis skipped as per configuration.\")\n",
        "        return pd.DataFrame(), pd.Series()\n",
        "\n",
        "    # --- Step 5.1: Generate Parameter Grid and Scenario Combinations ---\n",
        "    prod_ext_factors = sa_config['parameter_grid']['production_extension_factors']\n",
        "    trade_flex_factors = sa_config['parameter_grid']['trade_flexibility_factors']\n",
        "\n",
        "    # Create the Cartesian product of the parameter grids.\n",
        "    parameter_combinations = list(itertools.product(prod_ext_factors, trade_flex_factors))\n",
        "    num_scenarios = len(parameter_combinations)\n",
        "    print(f\"Generated {num_scenarios} scenarios for the sensitivity analysis.\")\n",
        "\n",
        "    # --- Step 5.2: Systematic Scenario Execution ---\n",
        "    # This list will store the detailed results of each run.\n",
        "    raw_results_list = []\n",
        "\n",
        "    # Use tqdm for a progress bar during the simulation loop.\n",
        "    for ext_factor, flex_factor in tqdm(parameter_combinations, desc=\"Running Sensitivity Scenarios\"):\n",
        "        # Construct the specific configuration for this iteration.\n",
        "        scenario_config = {\n",
        "            'name': f\"Ext={ext_factor*100}%, Flex={flex_factor*100}%\",\n",
        "            'disruption': sa_config['disruption_scenario'],\n",
        "            'extension_factor': ext_factor,\n",
        "            'flexibility_factor': flex_factor\n",
        "        }\n",
        "\n",
        "        # Execute the core MRIA algorithm for this single scenario.\n",
        "        solution = run_mria_core_algorithm(data, alpha, scenario_config)\n",
        "\n",
        "        # Store the parameters and the full solution object.\n",
        "        raw_results_list.append({\n",
        "            'params': {'extension_factor': ext_factor, 'flexibility_factor': flex_factor},\n",
        "            'solution': solution\n",
        "        })\n",
        "\n",
        "    print(\"...All scenarios executed successfully.\")\n",
        "\n",
        "    # --- Step 5.3: Aggregate Results ---\n",
        "    print(\"Aggregating results into a summary DataFrame...\")\n",
        "    aggregated_results_df = _aggregate_sensitivity_results(raw_results_list, data)\n",
        "    print(\"...Aggregation complete.\")\n",
        "\n",
        "    # --- Step 5.3.3: Perform Trade Dependency Analysis ---\n",
        "    print(\"Calculating trade dependency ratios...\")\n",
        "    disruption_info = sa_config['disruption_scenario']\n",
        "    dependency_ratios = _calculate_trade_dependency_ratio(\n",
        "        data,\n",
        "        target_region=disruption_info['target_region_code'],\n",
        "        target_sectors=disruption_info['target_sectors']\n",
        "    )\n",
        "    print(\"...Trade dependency analysis complete.\")\n",
        "\n",
        "    print(\"--- Task 5 Successfully Completed: Sensitivity analysis finished. ---\")\n",
        "\n",
        "    return aggregated_results_df, dependency_ratios\n"
      ],
      "metadata": {
        "id": "NG0zDEA1yp9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Criticality Analysis Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Criticality Analysis Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Criticality Score and Metric Calculation (Sub-Task 6.2)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _calculate_criticality_metrics(\n",
        "    rationing_results: Dict[Tuple[str, str], float],\n",
        "    data: 'PreprocessedData'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates criticality scores and other metrics from stress test results.\n",
        "\n",
        "    This function processes the raw rationing data from the sectoral stress tests\n",
        "    to compute the normalized rationing-based criticality score (Eq. 5), the\n",
        "    normalized economic output, and the Location Quotient (Eq. 6) for each\n",
        "    sector.\n",
        "\n",
        "    Args:\n",
        "        rationing_results (Dict[Tuple[str, str], float]): A dictionary mapping\n",
        "            (region, sector) tuples to the total system-wide rationing caused\n",
        "            by their disruption.\n",
        "        data (PreprocessedData): The preprocessed data dictionary, needed for\n",
        "                                 baseline production values.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a (region, sector) MultiIndex containing\n",
        "                      the calculated criticality metrics.\n",
        "    \"\"\"\n",
        "    # Convert the rationing results dictionary to a pandas Series for easier manipulation.\n",
        "    rationing_series = pd.Series(rationing_results).rename(\"total_rationing\")\n",
        "\n",
        "    # --- 1. Rationing-Based Criticality Score (Eq. 5) ---\n",
        "    # c_r,s = R_r,s / sum_r,s(R_r,s)\n",
        "    # Calculate the total sum of rationing across all stress tests (the denominator).\n",
        "    total_rationing_sum = rationing_series.sum()\n",
        "    # Normalize each sector's rationing value to get the criticality score.\n",
        "    # Handle the case where total rationing is zero to avoid division by zero.\n",
        "    if total_rationing_sum > 1e-9:\n",
        "        criticality_score = (rationing_series / total_rationing_sum).rename(\"criticality_score\")\n",
        "    else:\n",
        "        criticality_score = pd.Series(0.0, index=rationing_series.index, name=\"criticality_score\")\n",
        "\n",
        "    # --- 2. Normalized Economic Output ---\n",
        "    # output_score_r,s = x_bar_r,s / sum_r,s(x_bar_r,s)\n",
        "    # Convert the baseline production matrix to a pandas Series.\n",
        "    x_bar_df = pd.DataFrame(\n",
        "        data['baseline_production'],\n",
        "        index=data['idx_to_region'].values(),\n",
        "        columns=data['idx_to_sector'].values()\n",
        "    )\n",
        "    x_bar_series = x_bar_df.stack()\n",
        "    # Calculate the total national production (the denominator).\n",
        "    total_national_production = x_bar_series.sum()\n",
        "    # Normalize each sector's production.\n",
        "    if total_national_production > 1e-9:\n",
        "        normalized_output = (x_bar_series / total_national_production).rename(\"normalized_output\")\n",
        "    else:\n",
        "        normalized_output = pd.Series(0.0, index=x_bar_series.index, name=\"normalized_output\")\n",
        "\n",
        "    # --- 3. Location Quotient (LQ) (Eq. 6) ---\n",
        "    # LQ_r,s = (x_r,s / X_r) / (X_s / X)\n",
        "    # Add a small epsilon for numerical stability in denominators.\n",
        "    epsilon = 1e-9\n",
        "    # Calculate regional totals (X_r = sum_s(x_r,s))\n",
        "    regional_totals = x_bar_series.groupby(level=0).sum()\n",
        "    # Calculate national sectoral totals (X_s = sum_r(x_r,s))\n",
        "    sectoral_totals = x_bar_series.groupby(level=1).sum()\n",
        "    # The national total (X) is already calculated.\n",
        "\n",
        "    # Calculate the numerator: (x_r,s / X_r)\n",
        "    numerator = x_bar_series / regional_totals.reindex(x_bar_series.index, level=0)\n",
        "    # Calculate the denominator: (X_s / X)\n",
        "    denominator = sectoral_totals / (total_national_production + epsilon)\n",
        "    # Compute the Location Quotient.\n",
        "    location_quotient = (numerator / denominator.reindex(x_bar_series.index, level=1)).rename(\"location_quotient\")\n",
        "\n",
        "    # --- Assemble the Final DataFrame ---\n",
        "    # Combine all calculated series into a single DataFrame.\n",
        "    metrics_df = pd.concat([\n",
        "        criticality_score,\n",
        "        normalized_output,\n",
        "        location_quotient,\n",
        "        x_bar_series.rename(\"baseline_production\")\n",
        "    ], axis=1)\n",
        "\n",
        "    # Ensure the index has the correct names.\n",
        "    metrics_df.index.names = ['region', 'sector']\n",
        "\n",
        "    return metrics_df.fillna(0)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6 Orchestrator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_criticality_analysis(\n",
        "    data: 'PreprocessedData',\n",
        "    alpha: float,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full criticality analysis as described in Section 2.4.2.\n",
        "\n",
        "    This function systematically stress-tests each economically active sector\n",
        "    in the economy. For each sector, it applies a fixed disruption and simulates\n",
        "    the system's response under a flexible configuration. It then calculates\n",
        "    criticality scores based on the resulting system-wide rationing.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary from Task 2.\n",
        "        alpha (float): The calibrated trade cost parameter from Task 3.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the criticality metrics (criticality\n",
        "                      score, normalized output, location quotient) for every\n",
        "                      region-sector pair.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Task 6: Criticality Analysis ---\")\n",
        "\n",
        "    # Extract the relevant section of the configuration.\n",
        "    ca_config = config['criticality_analysis']\n",
        "    if not ca_config['run_analysis']:\n",
        "        print(\"Criticality analysis skipped as per configuration.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 6.1.1: Enumerate Target Sectors for Stress Testing ---\n",
        "    # Identify all (region, sector) pairs with non-negligible production.\n",
        "    target_sectors_to_test = []\n",
        "    for r_idx, r_code in data['idx_to_region'].items():\n",
        "        for s_idx, s_code in data['idx_to_sector'].items():\n",
        "            if data['baseline_production'][r_idx, s_idx] > 1e-6:\n",
        "                target_sectors_to_test.append((r_code, s_code))\n",
        "\n",
        "    print(f\"Identified {len(target_sectors_to_test)} economically active sectors for stress testing.\")\n",
        "\n",
        "    # --- Step 6.1.3: Execute Stress Testing Loop ---\n",
        "    # This dictionary will store the results: (region, sector) -> total_rationing.\n",
        "    rationing_results: Dict[Tuple[str, str], float] = {}\n",
        "\n",
        "    # Define the fixed parameters for the \"flexible system\".\n",
        "    flexible_params = ca_config['flexible_system_parameters']\n",
        "\n",
        "    # Use tqdm for a progress bar.\n",
        "    for r_target, s_target in tqdm(target_sectors_to_test, desc=\"Running Criticality Stress Tests\"):\n",
        "        # --- Step 6.1.2: Configure Individual Sector Disruption ---\n",
        "        # Create the specific scenario config for this single stress test.\n",
        "        scenario_config = {\n",
        "            'name': f\"Disruption on {r_target}-{s_target}\",\n",
        "            'disruption': {\n",
        "                'region': r_target,\n",
        "                'sectors': [s_target],\n",
        "                'magnitude': ca_config['disruption_magnitude']\n",
        "            },\n",
        "            'extension_factor': flexible_params['production_extension_factor'],\n",
        "            'flexibility_factor': flexible_params['trade_flexibility_factor']\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Execute the core MRIA algorithm for this scenario.\n",
        "            solution = run_mria_core_algorithm(data, alpha, scenario_config)\n",
        "\n",
        "            # Store the total system-wide rationing.\n",
        "            total_rationing = np.sum(solution['min_rationing_vector'])\n",
        "            rationing_results[(r_target, s_target)] = total_rationing\n",
        "        except Exception as e:\n",
        "            # Log errors without halting the entire analysis.\n",
        "            print(f\"\\nERROR: Simulation failed for sector ({r_target}, {s_target}). Reason: {e}\")\n",
        "            rationing_results[(r_target, s_target)] = np.nan # Mark as failed\n",
        "\n",
        "    print(\"...All stress tests executed.\")\n",
        "\n",
        "    # --- Step 6.2: Calculate Final Criticality Metrics ---\n",
        "    print(\"Calculating criticality scores and other metrics...\")\n",
        "    criticality_metrics_df = _calculate_criticality_metrics(rationing_results, data)\n",
        "    print(\"...Metrics calculation complete.\")\n",
        "\n",
        "    print(\"--- Task 6 Successfully Completed: Criticality analysis finished. ---\")\n",
        "\n",
        "    return criticality_metrics_df\n"
      ],
      "metadata": {
        "id": "4al6gJ-YzWc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Incremental Disruption Analysis Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Incremental Disruption Analysis Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "def run_incremental_disruption_analysis(\n",
        "    data: 'PreprocessedData',\n",
        "    alpha: float,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the incremental disruption analysis as per Section 2.4.3.\n",
        "\n",
        "    This function systematically analyzes the impact of escalating disruptions on\n",
        "    specific target sectors under different system flexibility configurations\n",
        "    (rigid vs. flexible). It traces the value of rationed products as the\n",
        "    disruption magnitude increases from 1% to 100%, revealing critical\n",
        "    thresholds and system response patterns.\n",
        "\n",
        "    Args:\n",
        "        data (PreprocessedData): The preprocessed data dictionary from Task 2.\n",
        "        alpha (float): The calibrated trade cost parameter from Task 3.\n",
        "        config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the results. It has a MultiIndex\n",
        "                      of ('target_name', 'system_type', 'disruption_level')\n",
        "                      and a column for the total value of rationed products.\n",
        "                      This format is optimized for plotting and comparison.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Task 7: Incremental Disruption Analysis ---\")\n",
        "\n",
        "    # Extract the relevant section of the configuration.\n",
        "    ida_config = config['incremental_disruption_analysis']\n",
        "    if not ida_config['run_analysis']:\n",
        "        print(\"Incremental disruption analysis skipped as per configuration.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 7.1: Systematic Scenario Generation and Execution ---\n",
        "    # This list will store the results from each individual simulation run.\n",
        "    results_records = []\n",
        "\n",
        "    # Define the total number of iterations for the progress bar.\n",
        "    total_iterations = (\n",
        "        len(ida_config['target_scenarios']) *\n",
        "        len(ida_config['system_configurations']) *\n",
        "        len(ida_config['disruption_levels'])\n",
        "    )\n",
        "\n",
        "    # Initialize the progress bar.\n",
        "    with tqdm(total=total_iterations, desc=\"Running Incremental Disruption Scenarios\") as pbar:\n",
        "        # Outer Loop: Iterate through the target sectors (e.g., Chemicals, Petroleum).\n",
        "        for target_scenario in ida_config['target_scenarios']:\n",
        "            target_name = target_scenario['name']\n",
        "\n",
        "            # Middle Loop: Iterate through system types (e.g., 'rigid', 'flexible').\n",
        "            for system_type, system_params in ida_config['system_configurations'].items():\n",
        "\n",
        "                # Inner Loop: Iterate through escalating disruption levels.\n",
        "                for disruption_level in ida_config['disruption_levels']:\n",
        "                    # --- Construct the precise scenario configuration for this run ---\n",
        "                    scenario_config = {\n",
        "                        'name': f\"{target_name} | {system_type} | {disruption_level*100:.0f}% disruption\",\n",
        "                        'disruption': {\n",
        "                            'region': target_scenario['target_region_code'],\n",
        "                            'sectors': [target_scenario['target_sector_code']],\n",
        "                            'magnitude': disruption_level\n",
        "                        },\n",
        "                        'extension_factor': system_params['production_extension_factor'],\n",
        "                        'flexibility_factor': system_params['trade_flexibility_factor']\n",
        "                    }\n",
        "\n",
        "                    try:\n",
        "                        # --- Execute the core 3-step MRIA algorithm ---\n",
        "                        solution = run_mria_core_algorithm(data, alpha, scenario_config)\n",
        "\n",
        "                        # --- Collect the key result metric ---\n",
        "                        # For this analysis, we are primarily interested in the total value of rationed products.\n",
        "                        total_rationing = np.sum(solution['min_rationing_vector'])\n",
        "\n",
        "                        # --- Store the result in a structured record ---\n",
        "                        results_records.append({\n",
        "                            'target_name': target_name,\n",
        "                            'system_type': system_type,\n",
        "                            'disruption_level': disruption_level,\n",
        "                            'total_rationing': total_rationing\n",
        "                        })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # Log any errors and store a NaN to indicate failure for this point.\n",
        "                        print(f\"\\nERROR: Simulation failed for {scenario_config['name']}. Reason: {e}\")\n",
        "                        results_records.append({\n",
        "                            'target_name': target_name,\n",
        "                            'system_type': system_type,\n",
        "                            'disruption_level': disruption_level,\n",
        "                            'total_rationing': np.nan\n",
        "                        })\n",
        "\n",
        "                    # Update the progress bar after each simulation.\n",
        "                    pbar.update(1)\n",
        "\n",
        "    print(\"...All incremental disruption scenarios executed.\")\n",
        "\n",
        "    # --- Step 7.2: Aggregate Results into a Final DataFrame ---\n",
        "    print(\"Aggregating results into a final structured DataFrame...\")\n",
        "    if not results_records:\n",
        "        print(\"No results to aggregate.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert the list of dictionary records into a DataFrame.\n",
        "    results_df = pd.DataFrame.from_records(results_records)\n",
        "\n",
        "    # Set a hierarchical index for easy slicing, grouping, and analysis.\n",
        "    results_df.set_index(['target_name', 'system_type', 'disruption_level'], inplace=True)\n",
        "\n",
        "    # Sort the index to ensure disruption levels are in ascending order for plotting.\n",
        "    results_df.sort_index(inplace=True)\n",
        "\n",
        "    print(\"...Aggregation complete.\")\n",
        "    print(\"--- Task 7 Successfully Completed: Incremental disruption analysis finished. ---\")\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "5CyRVEKbz7bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Research Pipeline Orchestrator Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Research Pipeline Orchestrator Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "def run_mria_pipeline(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end MRIA research pipeline.\n",
        "\n",
        "    This master function serves as the single entry point for the entire\n",
        "    analysis. It executes a sequence of modular steps:\n",
        "    1.  Validates all input data and configurations.\n",
        "    2.  Preprocesses the data into numerical formats for the model.\n",
        "    3.  Calibrates the crucial 'alpha' trade cost parameter.\n",
        "    4.  Conditionally runs the Sensitivity, Criticality, and/or Incremental\n",
        "        Disruption analyses based on the provided configuration.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production values.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade flows.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply technical coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use technical coefficients.\n",
        "        config (Dict[str, Any]): The master configuration dictionary that\n",
        "                                 governs all aspects of the pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the comprehensive results from\n",
        "                        all executed analyses. The keys correspond to the\n",
        "                        analysis type (e.g., 'sensitivity_analysis_results').\n",
        "    \"\"\"\n",
        "    # Announce the start of the entire pipeline.\n",
        "    print(\"==========================================================\")\n",
        "    print(\"===   STARTING MULTI-REGIONAL IMPACT ASSESSMENT (MRIA) PIPELINE   ===\")\n",
        "    print(\"==========================================================\")\n",
        "\n",
        "    # This dictionary will store all results generated by the pipeline.\n",
        "    pipeline_results: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # --- STAGE 1: INPUT DATA VALIDATION ---\n",
        "        # This is a critical first step. If the inputs are invalid,\n",
        "        # the pipeline halts immediately with a descriptive error.\n",
        "        validated_sets = run_input_validation_and_quality_assurance(\n",
        "            initial_production, initial_trade, supply_coefficients, use_coefficients, config\n",
        "        )\n",
        "\n",
        "        # --- STAGE 2: DATA PREPROCESSING ---\n",
        "        # Convert the validated DataFrames into numpy arrays and mappings.\n",
        "        # This is done once and the result is reused by all subsequent stages.\n",
        "        preprocessed_data = preprocess_data_for_mria(\n",
        "            initial_production, initial_trade, supply_coefficients, use_coefficients, validated_sets\n",
        "        )\n",
        "        pipeline_results['preprocessed_data'] = preprocessed_data\n",
        "\n",
        "        # --- STAGE 3: MODEL CALIBRATION ---\n",
        "        # Calibrate the alpha parameter using the baseline economic data.\n",
        "        # This is also done once and reused.\n",
        "        calibrated_alpha = calibrate_alpha_parameter(preprocessed_data, config)\n",
        "        pipeline_results['calibrated_alpha'] = calibrated_alpha\n",
        "\n",
        "        # --- STAGE 4: CONDITIONAL ANALYSIS EXECUTION ---\n",
        "        # Each analysis is run only if its flag is set to True in the config.\n",
        "\n",
        "        # --- Sensitivity Analysis (Task 5) ---\n",
        "        if config.get('sensitivity_analysis', {}).get('run_analysis', False):\n",
        "            sa_results, sa_dependency_ratios = run_sensitivity_analysis(\n",
        "                preprocessed_data, calibrated_alpha, config\n",
        "            )\n",
        "            pipeline_results['sensitivity_analysis_results'] = sa_results\n",
        "            pipeline_results['sensitivity_dependency_ratios'] = sa_dependency_ratios\n",
        "        else:\n",
        "            print(\"\\nSkipping Sensitivity Analysis as per configuration.\")\n",
        "\n",
        "        # --- Criticality Analysis (Task 6) ---\n",
        "        if config.get('criticality_analysis', {}).get('run_analysis', False):\n",
        "            crit_results = run_criticality_analysis(\n",
        "                preprocessed_data, calibrated_alpha, config\n",
        "            )\n",
        "            pipeline_results['criticality_analysis_results'] = crit_results\n",
        "        else:\n",
        "            print(\"\\nSkipping Criticality Analysis as per configuration.\")\n",
        "\n",
        "        # --- Incremental Disruption Analysis (Task 7) ---\n",
        "        if config.get('incremental_disruption_analysis', {}).get('run_analysis', False):\n",
        "            ida_results = run_incremental_disruption_analysis(\n",
        "                preprocessed_data, calibrated_alpha, config\n",
        "            )\n",
        "            pipeline_results['incremental_disruption_results'] = ida_results\n",
        "        else:\n",
        "            print(\"\\nSkipping Incremental Disruption Analysis as per configuration.\")\n",
        "\n",
        "        # Announce the successful completion of the pipeline.\n",
        "        print(\"\\n==========================================================\")\n",
        "        print(\"===      MRIA PIPELINE COMPLETED SUCCESSFULLY      ===\")\n",
        "        print(\"==========================================================\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception that occurs anywhere in the pipeline.\n",
        "        print(\"\\n==========================================================\")\n",
        "        print(\"===         !!! MRIA PIPELINE FAILED !!!         ===\")\n",
        "        print(\"==========================================================\")\n",
        "        # Print a detailed error message.\n",
        "        print(f\"An error occurred during pipeline execution: {e}\")\n",
        "        # Re-raise the exception to provide a full traceback for debugging.\n",
        "        raise\n",
        "\n",
        "    # Return the dictionary of all collected results.\n",
        "    return pipeline_results\n"
      ],
      "metadata": {
        "id": "WzpdPDDx0ev5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Robustness Analysis Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Variant 1: Alpha Parameter Robustness Analysis\n",
        "# ==============================================================================\n",
        "\n",
        "def run_alpha_robustness_analysis(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    alpha_grid: List[float] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Conducts a robustness analysis on the 'alpha' trade cost parameter.\n",
        "\n",
        "    This function systematically re-runs the entire MRIA criticality analysis\n",
        "    for a range of different alpha values. It then aggregates the results and\n",
        "    calculates the stability of the sectoral criticality rankings to assess\n",
        "    the model's sensitivity to this key parameter.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use coefficients.\n",
        "        base_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        alpha_grid (List[float], optional): A list of alpha values to test.\n",
        "            If None, a default grid is used. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the aggregated criticality results\n",
        "                      for all tested alpha values, including the original scores,\n",
        "                      ranks for each alpha, and the Spearman rank correlation\n",
        "                      with the baseline alpha's rankings.\n",
        "    \"\"\"\n",
        "    # Announce the start of this specific robustness task.\n",
        "    print(\"\\n==========================================================\")\n",
        "    print(\"===   STARTING TASK 9.1.1: ALPHA ROBUSTNESS ANALYSIS   ===\")\n",
        "    print(\"==========================================================\")\n",
        "\n",
        "    # --- Step 1: Setup the Analysis Grid and Configuration ---\n",
        "    # Define the grid of alpha values to test if not provided.\n",
        "    if alpha_grid is None:\n",
        "        alpha_grid = [1.0, 1.15, 1.25, 1.35, 1.5, 1.75, 2.0]\n",
        "\n",
        "    # Identify the baseline alpha value from the config for later comparison.\n",
        "    baseline_alpha = base_config['general_parameters']['alpha_trade_cost']\n",
        "    if baseline_alpha not in alpha_grid:\n",
        "        alpha_grid.append(baseline_alpha)\n",
        "        alpha_grid.sort()\n",
        "\n",
        "    # This list will store the criticality result DataFrame from each run.\n",
        "    all_results = []\n",
        "\n",
        "    # --- Step 2: Loop Through Alpha Grid and Run Pipeline ---\n",
        "    for alpha_val in alpha_grid:\n",
        "        print(f\"\\n--- Running pipeline for alpha = {alpha_val:.2f} ---\")\n",
        "\n",
        "        # Create a deep copy of the base config to avoid modifying the original.\n",
        "        run_config = copy.deepcopy(base_config)\n",
        "\n",
        "        # Modify the alpha parameter for this specific run.\n",
        "        run_config['general_parameters']['alpha_trade_cost'] = alpha_val\n",
        "\n",
        "        # For this specific robustness test, we only need to run the\n",
        "        # criticality analysis. Disable other analyses to save time.\n",
        "        run_config['sensitivity_analysis']['run_analysis'] = False\n",
        "        run_config['criticality_analysis']['run_analysis'] = True\n",
        "        run_config['incremental_disruption_analysis']['run_analysis'] = False\n",
        "\n",
        "        try:\n",
        "            # Execute the entire pipeline with the modified configuration.\n",
        "            # The pipeline will handle validation, preprocessing, and calibration\n",
        "            # (though the calibrated alpha will be overwritten by our loop).\n",
        "            pipeline_results = run_mria_pipeline(\n",
        "                initial_production, initial_trade,\n",
        "                supply_coefficients, use_coefficients,\n",
        "                run_config\n",
        "            )\n",
        "\n",
        "            # Extract the criticality results DataFrame.\n",
        "            crit_results = pipeline_results['criticality_analysis_results']\n",
        "\n",
        "            # Add a column to identify which alpha value produced these results.\n",
        "            crit_results['alpha_value'] = alpha_val\n",
        "\n",
        "            # Append the results to our master list.\n",
        "            all_results.append(crit_results)\n",
        "\n",
        "            print(f\"--- Successfully completed run for alpha = {alpha_val:.2f} ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log errors for a specific alpha run without halting the entire analysis.\n",
        "            print(f\"\\nERROR: Pipeline run failed for alpha = {alpha_val}. Reason: {e}\")\n",
        "            print(\"Skipping this alpha value and continuing robustness analysis.\")\n",
        "\n",
        "    # --- Step 3: Aggregate and Analyze Results ---\n",
        "    if not all_results:\n",
        "        # Handle the case where all runs failed.\n",
        "        print(\"Robustness analysis failed: no successful pipeline runs.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate all individual result DataFrames into a single DataFrame.\n",
        "    aggregated_df = pd.concat(all_results)\n",
        "\n",
        "    # --- Step 4: Calculate Rank Stability ---\n",
        "    print(\"\\n--- Analyzing rank stability across alpha values ---\")\n",
        "\n",
        "    # Calculate the criticality rank for each sector within each alpha group.\n",
        "    # Higher score = higher rank (rank 1).\n",
        "    aggregated_df['rank'] = aggregated_df.groupby('alpha_value')['criticality_score'].rank(ascending=False, method='min')\n",
        "\n",
        "    # Pivot the table to have ranks in columns for easy correlation calculation.\n",
        "    rank_pivot = aggregated_df.pivot_table(\n",
        "        index=['region', 'sector'],\n",
        "        columns='alpha_value',\n",
        "        values='rank'\n",
        "    )\n",
        "\n",
        "    # Get the rankings from the baseline alpha run.\n",
        "    baseline_ranks = rank_pivot[baseline_alpha]\n",
        "\n",
        "    # Calculate the Spearman rank correlation between the baseline and each other run.\n",
        "    rank_correlations = {}\n",
        "    for alpha_val in rank_pivot.columns:\n",
        "        if alpha_val != baseline_alpha:\n",
        "            # Compare the current alpha's ranks against the baseline ranks.\n",
        "            corr, p_value = spearmanr(baseline_ranks, rank_pivot[alpha_val])\n",
        "            rank_correlations[alpha_val] = corr\n",
        "            print(f\"  Spearman correlation with baseline (alpha={baseline_alpha}) for alpha={alpha_val}: {corr:.4f}\")\n",
        "\n",
        "    # Add the correlation results to the main DataFrame for a comprehensive output.\n",
        "    # This is a convenient way to store and view the stability metric.\n",
        "    aggregated_df['rank_correlation_with_baseline'] = aggregated_df['alpha_value'].map(rank_correlations)\n",
        "\n",
        "    print(\"\\n==========================================================\")\n",
        "    print(\"===    ALPHA ROBUSTNESS ANALYSIS COMPLETED    ===\")\n",
        "    print(\"==========================================================\")\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Variant 2: Disruption Magnitude Robustness Analysis\n",
        "# ==============================================================================\n",
        "\n",
        "def run_disruption_magnitude_robustness_analysis(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    disruption_grid: List[float] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Conducts a robustness analysis on the disruption magnitude parameter.\n",
        "\n",
        "    This function systematically re-runs the entire MRIA sensitivity analysis\n",
        "    (the 18-scenario parameter grid) for a range of different initial disruption\n",
        "    magnitudes. It assesses whether the qualitative relationships between\n",
        "    production extension, trade flexibility, and economic impact are stable\n",
        "    under varying levels of initial shock.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use coefficients.\n",
        "        base_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        disruption_grid (List[float], optional): A list of disruption magnitudes\n",
        "            (e.g., 0.1 for 10%) to test. If None, a default grid is used.\n",
        "            Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A comprehensive DataFrame containing the aggregated\n",
        "                      sensitivity analysis results for all tested disruption\n",
        "                      magnitudes.\n",
        "    \"\"\"\n",
        "    # Announce the start of this specific robustness task.\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===   STARTING TASK 9.1.2: DISRUPTION MAGNITUDE ROBUSTNESS ANALYSIS   ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    # --- Step 1: Setup the Analysis Grid and Configuration ---\n",
        "    # Define the grid of disruption magnitudes to test if not provided.\n",
        "    if disruption_grid is None:\n",
        "        disruption_grid = [0.05, 0.10, 0.15, 0.20]\n",
        "\n",
        "    # This list will store the sensitivity analysis result DataFrame from each run.\n",
        "    all_results = []\n",
        "\n",
        "    # --- Step 2: Loop Through Disruption Grid and Run Pipeline ---\n",
        "    # Use tqdm for a high-level progress bar over the disruption magnitudes.\n",
        "    for magnitude in tqdm(disruption_grid, desc=\"Testing Disruption Magnitudes\"):\n",
        "        print(f\"\\n--- Running full sensitivity analysis for disruption magnitude = {magnitude*100:.1f}% ---\")\n",
        "\n",
        "        # Create a deep copy of the base config to ensure no side effects.\n",
        "        run_config = copy.deepcopy(base_config)\n",
        "\n",
        "        # Modify the disruption magnitude for this specific run.\n",
        "        run_config['sensitivity_analysis']['disruption_scenario']['disruption_magnitude'] = magnitude\n",
        "\n",
        "        # For this robustness test, we only need to run the sensitivity analysis.\n",
        "        # Disable other analyses to optimize computation time.\n",
        "        run_config['sensitivity_analysis']['run_analysis'] = True\n",
        "        run_config['criticality_analysis']['run_analysis'] = False\n",
        "        run_config['incremental_disruption_analysis']['run_analysis'] = False\n",
        "\n",
        "        try:\n",
        "            # Execute the entire pipeline with the modified configuration.\n",
        "            # This will run the full 18-scenario sensitivity analysis internally.\n",
        "            pipeline_results = run_mria_pipeline(\n",
        "                initial_production, initial_trade,\n",
        "                supply_coefficients, use_coefficients,\n",
        "                run_config\n",
        "            )\n",
        "\n",
        "            # Extract the sensitivity analysis results DataFrame.\n",
        "            sa_results = pipeline_results.get('sensitivity_analysis_results')\n",
        "\n",
        "            if sa_results is not None and not sa_results.empty:\n",
        "                # Add a column to identify which disruption magnitude produced these results.\n",
        "                sa_results['disruption_magnitude'] = magnitude\n",
        "\n",
        "                # Append the results to our master list.\n",
        "                all_results.append(sa_results)\n",
        "                print(f\"--- Successfully completed run for disruption magnitude = {magnitude*100:.1f}% ---\")\n",
        "            else:\n",
        "                print(f\"Warning: No sensitivity analysis results returned for magnitude {magnitude*100:.1f}%.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log errors for a specific magnitude run without halting the entire analysis.\n",
        "            print(f\"\\nERROR: Pipeline run failed for disruption magnitude = {magnitude}. Reason: {e}\")\n",
        "            print(\"Skipping this magnitude and continuing robustness analysis.\")\n",
        "\n",
        "    # --- Step 3: Aggregate and Finalize Results ---\n",
        "    if not all_results:\n",
        "        # Handle the case where all runs failed.\n",
        "        print(\"Robustness analysis failed: no successful pipeline runs.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate all individual sensitivity analysis DataFrames into a single,\n",
        "    # comprehensive DataFrame for multi-dimensional analysis.\n",
        "    aggregated_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    # Set a clear index for easy analysis and interpretation.\n",
        "    aggregated_df.set_index([\n",
        "        'disruption_magnitude',\n",
        "        'trade_flexibility_factor',\n",
        "        'production_extension_factor'\n",
        "    ], inplace=True)\n",
        "    aggregated_df.sort_index(inplace=True)\n",
        "\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===    DISRUPTION MAGNITUDE ROBUSTNESS ANALYSIS COMPLETED    ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Variant 3: Structural Robustness via Coefficient Perturbation\n",
        "# ==============================================================================\n",
        "\n",
        "def _run_single_perturbation_iteration(\n",
        "    iteration_num: int,\n",
        "    base_data_tuple: Tuple,\n",
        "    base_config: Dict[str, Any],\n",
        "    perturbation_factor: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Helper function to run a single Monte Carlo iteration in parallel.\n",
        "\n",
        "    This function perturbs the coefficient matrices, runs the criticality\n",
        "    analysis, and returns the results for one iteration. It is designed to be\n",
        "    called by a parallel executor.\n",
        "\n",
        "    Args:\n",
        "        iteration_num (int): The identifier for this iteration.\n",
        "        base_data_tuple (Tuple): A tuple of the raw dataframes to avoid\n",
        "                                 large object serialization issues.\n",
        "        base_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        perturbation_factor (float): The magnitude of the random shock (e.g., 0.05 for +/- 5%).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The criticality analysis results for this iteration.\n",
        "    \"\"\"\n",
        "    # Unpack raw data\n",
        "    initial_production, initial_trade, supply_coefficients, use_coefficients = base_data_tuple\n",
        "\n",
        "    # --- 1. Create a perturbed copy of the data ---\n",
        "    # Deepcopy the coefficient dataframes to avoid modifying the originals.\n",
        "    sc_perturbed = supply_coefficients.copy()\n",
        "    uc_perturbed = use_coefficients.copy()\n",
        "\n",
        "    # Generate a uniform random shock for supply coefficients.\n",
        "    sc_shock = np.random.uniform(1 - perturbation_factor, 1 + perturbation_factor, size=len(sc_perturbed))\n",
        "    sc_perturbed['coefficient'] *= sc_shock\n",
        "\n",
        "    # Generate a uniform random shock for use coefficients.\n",
        "    uc_shock = np.random.uniform(1 - perturbation_factor, 1 + perturbation_factor, size=len(uc_perturbed))\n",
        "    uc_perturbed['coefficient'] *= uc_shock\n",
        "\n",
        "    # --- 2. Run the pipeline with perturbed data ---\n",
        "    # Create a deep copy of the config for this run.\n",
        "    run_config = copy.deepcopy(base_config)\n",
        "    # Isolate the criticality analysis.\n",
        "    run_config['sensitivity_analysis']['run_analysis'] = False\n",
        "    run_config['criticality_analysis']['run_analysis'] = True\n",
        "    run_config['incremental_disruption_analysis']['run_analysis'] = False\n",
        "    # Suppress verbose output from the main pipeline for cleaner parallel logs.\n",
        "    run_config['verbose'] = False\n",
        "\n",
        "    # Run the full pipeline from validation to analysis with the perturbed data.\n",
        "    pipeline_results = run_mria_pipeline(\n",
        "        initial_production, initial_trade,\n",
        "        sc_perturbed, uc_perturbed,\n",
        "        run_config\n",
        "    )\n",
        "\n",
        "    # Extract and return the results.\n",
        "    crit_results = pipeline_results['criticality_analysis_results']\n",
        "    crit_results['iteration'] = iteration_num\n",
        "    return crit_results\n",
        "\n",
        "\n",
        "def run_structural_robustness_analysis(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    num_iterations: int = 100,\n",
        "    perturbation_factor: float = 0.05,\n",
        "    random_seed: int = 42,\n",
        "    max_workers: int = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Conducts a structural robustness analysis via Monte Carlo simulation.\n",
        "\n",
        "    This function tests the model's sensitivity to input data uncertainty by\n",
        "    repeatedly running the criticality analysis on randomly perturbed versions\n",
        "    of the technical coefficient matrices. It then aggregates the results to\n",
        "    produce confidence intervals for each sector's criticality score.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use coefficients.\n",
        "        base_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        num_iterations (int): The number of Monte Carlo iterations to run.\n",
        "        perturbation_factor (float): The magnitude of the uniform random shock\n",
        "                                     (e.g., 0.05 for +/- 5%).\n",
        "        random_seed (int): A seed for the random number generator for reproducibility.\n",
        "        max_workers (int, optional): The maximum number of processes to use for\n",
        "                                     parallel execution. If None, uses all available cores.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary DataFrame with a (region, sector) MultiIndex,\n",
        "                      showing statistics (mean, std, quantiles) of the\n",
        "                      criticality scores across all Monte Carlo iterations.\n",
        "    \"\"\"\n",
        "    # Announce the start of this computationally intensive task.\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===   STARTING TASK 9.2.1: STRUCTURAL ROBUSTNESS ANALYSIS   ===\")\n",
        "    print(f\"===   (Monte Carlo with {num_iterations} iterations)   ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    # Set the random seed for reproducibility of the entire analysis.\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Package raw data for the parallel executor.\n",
        "    base_data_tuple = (initial_production, initial_trade, supply_coefficients, use_coefficients)\n",
        "\n",
        "    # This list will store the future objects from the parallel submissions.\n",
        "    futures = []\n",
        "    all_results = []\n",
        "\n",
        "    # --- Step 1: Execute Monte Carlo Simulations in Parallel ---\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all simulation jobs to the executor pool.\n",
        "        for i in range(num_iterations):\n",
        "            future = executor.submit(\n",
        "                _run_single_perturbation_iteration,\n",
        "                i, base_data_tuple, base_config, perturbation_factor\n",
        "            )\n",
        "            futures.append(future)\n",
        "\n",
        "        # Collect results as they complete, with a progress bar.\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=num_iterations, desc=\"Running MC Iterations\"):\n",
        "            try:\n",
        "                result_df = future.result()\n",
        "                all_results.append(result_df)\n",
        "            except Exception as e:\n",
        "                print(f\"\\nERROR: A Monte Carlo iteration failed: {e}\")\n",
        "\n",
        "    # --- Step 2: Aggregate and Summarize Results ---\n",
        "    if not all_results:\n",
        "        print(\"Structural robustness analysis failed: no successful iterations.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate all individual result DataFrames.\n",
        "    aggregated_df = pd.concat(all_results)\n",
        "\n",
        "    print(\"\\n--- Summarizing criticality score distributions ---\")\n",
        "\n",
        "    # Group by sector and calculate summary statistics for the criticality score.\n",
        "    summary_stats = aggregated_df.groupby(['region', 'sector'])['criticality_score'].agg(\n",
        "        mean_criticality='mean',\n",
        "        std_criticality='std',\n",
        "        p05_criticality=lambda x: x.quantile(0.05),\n",
        "        p25_criticality=lambda x: x.quantile(0.25),\n",
        "        median_criticality='median',\n",
        "        p75_criticality=lambda x: x.quantile(0.75),\n",
        "        p95_criticality=lambda x: x.quantile(0.95)\n",
        "    ).sort_values(by='mean_criticality', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 sectors by mean criticality score across simulations:\")\n",
        "    print(summary_stats.head(10))\n",
        "\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===    STRUCTURAL ROBUSTNESS ANALYSIS COMPLETED    ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    return summary_stats\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Variant 4: Numerical Precision and Solver Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def run_numerical_robustness_analysis(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    test_grid: List[Dict[str, Any]] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Conducts a robustness analysis on numerical precision settings.\n",
        "\n",
        "    This function re-runs the criticality analysis under different Gurobi solver\n",
        "    tolerance settings to ensure that the model's key outputs (specifically,\n",
        "    the criticality rankings) are stable and not artifacts of a specific\n",
        "    numerical precision level.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use coefficients.\n",
        "        base_config (Dict[str, Any]): The master configuration dictionary.\n",
        "        test_grid (List[Dict[str, Any]], optional): A list of solver option\n",
        "            dictionaries to test. If None, a default grid is used.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the rank correlation of criticality\n",
        "                      scores for each test setting compared to the baseline setting.\n",
        "    \"\"\"\n",
        "    # Announce the start of this specific robustness task.\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===   STARTING TASK 9.3.2: NUMERICAL ROBUSTNESS ANALYSIS   ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    # --- Step 1: Define the Test Grid ---\n",
        "    # Define the grid of solver configurations to test if not provided.\n",
        "    if test_grid is None:\n",
        "        test_grid = [\n",
        "            {'name': 'Loose_Tol', 'FeasibilityTol': 1e-6, 'OptimalityTol': 1e-6},\n",
        "            {'name': 'Default_Tol', 'FeasibilityTol': 1e-9, 'OptimalityTol': 1e-9}, # Baseline\n",
        "            {'name': 'Strict_Tol', 'FeasibilityTol': 1e-12, 'OptimalityTol': 1e-12},\n",
        "        ]\n",
        "\n",
        "    # This list will store the criticality result DataFrame from each run.\n",
        "    all_results = []\n",
        "\n",
        "    # --- Step 2: Loop Through Test Grid and Run Pipeline ---\n",
        "    for test_setting in test_grid:\n",
        "        setting_name = test_setting['name']\n",
        "        print(f\"\\n--- Running criticality analysis for setting: {setting_name} ---\")\n",
        "\n",
        "        # Create a deep copy of the base config.\n",
        "        run_config = copy.deepcopy(base_config)\n",
        "\n",
        "        # Add the special solver options key to the config.\n",
        "        # This requires the pipeline to be refactored to handle this key.\n",
        "        run_config['solver_options'] = test_setting\n",
        "\n",
        "        # Isolate the criticality analysis for this test.\n",
        "        run_config['sensitivity_analysis']['run_analysis'] = False\n",
        "        run_config['criticality_analysis']['run_analysis'] = True\n",
        "        run_config['incremental_disruption_analysis']['run_analysis'] = False\n",
        "\n",
        "        try:\n",
        "            # Execute the pipeline with the modified configuration.\n",
        "            pipeline_results = run_mria_pipeline(\n",
        "                initial_production, initial_trade,\n",
        "                supply_coefficients, use_coefficients,\n",
        "                run_config\n",
        "            )\n",
        "\n",
        "            # Extract the criticality results.\n",
        "            crit_results = pipeline_results['criticality_analysis_results']\n",
        "            crit_results['test_setting'] = setting_name\n",
        "            all_results.append(crit_results)\n",
        "            print(f\"--- Successfully completed run for setting: {setting_name} ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: Pipeline run failed for setting {setting_name}. Reason: {e}\")\n",
        "\n",
        "    # --- Step 3: Aggregate and Analyze Rank Stability ---\n",
        "    if not all_results:\n",
        "        print(\"Numerical robustness analysis failed: no successful runs.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate all results into a single DataFrame.\n",
        "    aggregated_df = pd.concat(all_results)\n",
        "\n",
        "    print(\"\\n--- Analyzing rank stability across numerical settings ---\")\n",
        "\n",
        "    # Pivot the table to get criticality scores in columns for comparison.\n",
        "    score_pivot = aggregated_df.pivot_table(\n",
        "        index=['region', 'sector'],\n",
        "        columns='test_setting',\n",
        "        values='criticality_score'\n",
        "    )\n",
        "\n",
        "    # Define the baseline setting for comparison.\n",
        "    baseline_setting = 'Default_Tol'\n",
        "    if baseline_setting not in score_pivot.columns:\n",
        "        print(f\"Warning: Baseline setting '{baseline_setting}' not found in results.\")\n",
        "        return score_pivot\n",
        "\n",
        "    baseline_scores = score_pivot[baseline_setting]\n",
        "\n",
        "    # Calculate Spearman rank correlation between baseline and other settings.\n",
        "    correlations = {}\n",
        "    for setting_name in score_pivot.columns:\n",
        "        corr, p_value = spearmanr(baseline_scores.rank(), score_pivot[setting_name].rank())\n",
        "        correlations[setting_name] = {'rank_correlation': corr, 'p_value': p_value}\n",
        "        print(f\"  Spearman correlation with baseline for '{setting_name}': {corr:.6f}\")\n",
        "\n",
        "    # Format the final output.\n",
        "    summary_df = pd.DataFrame.from_dict(correlations, orient='index')\n",
        "\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===    NUMERICAL ROBUSTNESS ANALYSIS COMPLETED    ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Variant 5: Alternative Optimization Formulation Testing\n",
        "# ==============================================================================\n",
        "\n",
        "def run_formulation_robustness_analysis(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    base_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, float]:\n",
        "    \"\"\"\n",
        "    Conducts a robustness analysis on the Step 1 objective formulation.\n",
        "\n",
        "    This function runs the criticality analysis twice: once with the baseline\n",
        "    unweighted rationing minimization, and once with an alternative,\n",
        "    welfare-weighted minimization. It then compares the resulting sectoral\n",
        "    criticality rankings to assess the model's robustness to this key\n",
        "    methodological choice.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use coefficients.\n",
        "        base_config (Dict[str, Any]): The master configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, float]: A tuple containing:\n",
        "        - The criticality results from the baseline formulation.\n",
        "        - The criticality results from the alternative formulation.\n",
        "        - The Spearman rank correlation between the two sets of rankings.\n",
        "    \"\"\"\n",
        "    # Announce the start of this specific robustness task.\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===   STARTING TASK 9.3.1: FORMULATION ROBUSTNESS ANALYSIS   ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    # This dictionary will store the results from the two runs.\n",
        "    results = {}\n",
        "\n",
        "    # Define the two formulations to be tested.\n",
        "    formulations_to_test = [\n",
        "        {'name': 'Baseline (Unweighted)', 'options': {'step1_objective': 'unweighted'}},\n",
        "        {'name': 'Alternative (Welfare-Weighted)', 'options': {'step1_objective': 'welfare_weighted'}}\n",
        "    ]\n",
        "\n",
        "    # --- Loop Through Formulations and Run Pipeline ---\n",
        "    for formulation in formulations_to_test:\n",
        "        name = formulation['name']\n",
        "        print(f\"\\n--- Running criticality analysis for formulation: {name} ---\")\n",
        "\n",
        "        # Create a deep copy of the base config.\n",
        "        run_config = copy.deepcopy(base_config)\n",
        "\n",
        "        # Add the special formulation options key to the config.\n",
        "        # This requires the pipeline to be refactored to handle this key.\n",
        "        run_config['formulation_options'] = formulation['options']\n",
        "\n",
        "        # Isolate the criticality analysis for this test.\n",
        "        run_config['sensitivity_analysis']['run_analysis'] = False\n",
        "        run_config['criticality_analysis']['run_analysis'] = True\n",
        "        run_config['incremental_disruption_analysis']['run_analysis'] = False\n",
        "\n",
        "        try:\n",
        "            # Execute the pipeline with the modified configuration.\n",
        "            pipeline_results = run_mria_pipeline(\n",
        "                initial_production, initial_trade,\n",
        "                supply_coefficients, use_coefficients,\n",
        "                run_config\n",
        "            )\n",
        "\n",
        "            # Store the criticality results.\n",
        "            results[name] = pipeline_results['criticality_analysis_results']\n",
        "            print(f\"--- Successfully completed run for formulation: {name} ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: Pipeline run failed for formulation {name}. Reason: {e}\")\n",
        "            results[name] = None\n",
        "\n",
        "    # --- Compare the Results ---\n",
        "    baseline_results = results.get('Baseline (Unweighted)')\n",
        "    alternative_results = results.get('Alternative (Welfare-Weighted)')\n",
        "\n",
        "    if baseline_results is None or alternative_results is None:\n",
        "        print(\"Analysis failed: one or both of the required runs did not complete.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), 0.0\n",
        "\n",
        "    print(\"\\n--- Analyzing rank stability across formulations ---\")\n",
        "\n",
        "    # Ensure both dataframes are aligned by index for comparison.\n",
        "    aligned_base, aligned_alt = baseline_results.align(alternative_results, join='inner', fill_value=0)\n",
        "\n",
        "    # Calculate the Spearman rank correlation between the two criticality score rankings.\n",
        "    corr, p_value = spearmanr(\n",
        "        aligned_base['criticality_score'],\n",
        "        aligned_alt['criticality_score']\n",
        "    )\n",
        "\n",
        "    print(f\"  Spearman rank correlation between formulations: {corr:.6f} (p-value: {p_value:.2e})\")\n",
        "\n",
        "    print(\"\\n=====================================================================\")\n",
        "    print(\"===    FORMULATION ROBUSTNESS ANALYSIS COMPLETED    ===\")\n",
        "    print(\"=====================================================================\")\n",
        "\n",
        "    return baseline_results, alternative_results, corr\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Master Robustness Analysis Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_robustness_analyses(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all specified robustness analyses.\n",
        "\n",
        "    This function serves as the master controller for the entire suite of\n",
        "    robustness tests defined in Task 9. It reads a dedicated 'robustness_analysis'\n",
        "    section from the configuration file and conditionally executes each enabled\n",
        "    analysis by calling its specialized orchestrator.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use coefficients.\n",
        "        config (Dict[str, Any]): The master configuration dictionary, which must\n",
        "                                 contain a 'robustness_analysis' key.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are the names of the\n",
        "                                 executed robustness analyses and values are the\n",
        "                                 resulting pandas DataFrames.\n",
        "    \"\"\"\n",
        "    # Announce the start of the entire robustness analysis suite.\n",
        "    print(\"\\n\\n==========================================================\")\n",
        "    print(\"===   STARTING MASTER ROBUSTNESS ANALYSIS ORCHESTRATOR   ===\")\n",
        "    print(\"==========================================================\")\n",
        "\n",
        "    # This dictionary will store all results from the various tests.\n",
        "    robustness_results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    # Extract the dedicated configuration for robustness analyses.\n",
        "    ra_config = config.get('robustness_analysis')\n",
        "    if not ra_config:\n",
        "        # If the key is missing, print a warning and return empty results.\n",
        "        print(\"Warning: 'robustness_analysis' key not found in config. Skipping all tests.\")\n",
        "        return robustness_results\n",
        "\n",
        "    # --- 1. Alpha Parameter Robustness Analysis ---\n",
        "    # Check the flag in the config to see if this analysis should be run.\n",
        "    if ra_config.get('run_alpha_robustness', False):\n",
        "        # Execute the dedicated function for this analysis.\n",
        "        alpha_results = run_alpha_robustness_analysis(\n",
        "            initial_production, initial_trade,\n",
        "            supply_coefficients, use_coefficients,\n",
        "            config, # Pass the full config\n",
        "            alpha_grid=ra_config.get('alpha_grid') # Pass the specific grid\n",
        "        )\n",
        "        # Store the results in the master dictionary.\n",
        "        robustness_results['alpha_robustness_results'] = alpha_results\n",
        "    else:\n",
        "        # Provide feedback if the analysis is skipped.\n",
        "        print(\"\\nSkipping Alpha Parameter Robustness Analysis as per configuration.\")\n",
        "\n",
        "    # --- 2. Disruption Magnitude Robustness Analysis ---\n",
        "    # Check the flag for this analysis.\n",
        "    if ra_config.get('run_disruption_magnitude_robustness', False):\n",
        "        # Execute the dedicated function.\n",
        "        disruption_mag_results = run_disruption_magnitude_robustness_analysis(\n",
        "            initial_production, initial_trade,\n",
        "            supply_coefficients, use_coefficients,\n",
        "            config,\n",
        "            disruption_grid=ra_config.get('disruption_grid')\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_results['disruption_magnitude_results'] = disruption_mag_results\n",
        "    else:\n",
        "        # Provide feedback.\n",
        "        print(\"\\nSkipping Disruption Magnitude Robustness Analysis as per configuration.\")\n",
        "\n",
        "    # --- 3. Structural Robustness (Coefficient Perturbation) Analysis ---\n",
        "    # Check the flag for this analysis.\n",
        "    if ra_config.get('run_structural_robustness', False):\n",
        "        # Extract parameters for this specific test.\n",
        "        structural_params = ra_config.get('structural_robustness_params', {})\n",
        "        # Execute the dedicated function.\n",
        "        structural_results = run_structural_robustness_analysis(\n",
        "            initial_production, initial_trade,\n",
        "            supply_coefficients, use_coefficients,\n",
        "            config,\n",
        "            num_iterations=structural_params.get('num_iterations', 50),\n",
        "            perturbation_factor=structural_params.get('perturbation_factor', 0.05),\n",
        "            random_seed=structural_params.get('random_seed', 42),\n",
        "            max_workers=structural_params.get('max_workers')\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_results['structural_robustness_results'] = structural_results\n",
        "    else:\n",
        "        # Provide feedback.\n",
        "        print(\"\\nSkipping Structural Robustness Analysis as per configuration.\")\n",
        "\n",
        "    # --- 4. Methodological Formulation Robustness Analysis ---\n",
        "    # Check the flag for this analysis.\n",
        "    if ra_config.get('run_formulation_robustness', False):\n",
        "        # Execute the dedicated function.\n",
        "        base_res, alt_res, corr = run_formulation_robustness_analysis(\n",
        "            initial_production, initial_trade,\n",
        "            supply_coefficients, use_coefficients,\n",
        "            config\n",
        "        )\n",
        "        # Store the results in a structured way.\n",
        "        robustness_results['formulation_robustness_baseline'] = base_res\n",
        "        robustness_results['formulation_robustness_alternative'] = alt_res\n",
        "        robustness_results['formulation_robustness_correlation'] = pd.DataFrame(\n",
        "            [{'spearman_rank_correlation': corr}]\n",
        "        )\n",
        "    else:\n",
        "        # Provide feedback.\n",
        "        print(\"\\nSkipping Formulation Robustness Analysis as per configuration.\")\n",
        "\n",
        "    # Announce the successful completion of the entire suite.\n",
        "    print(\"\\n==========================================================\")\n",
        "    print(\"===    ALL ROBUSTNESS ANALYSES COMPLETED    ===\")\n",
        "    print(\"==========================================================\")\n",
        "\n",
        "    # Return the dictionary containing all generated results.\n",
        "    return robustness_results\n"
      ],
      "metadata": {
        "id": "YlBfZpDKCNDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Master Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_full_experiment(\n",
        "    initial_production: pd.DataFrame,\n",
        "    initial_trade: pd.DataFrame,\n",
        "    supply_coefficients: pd.DataFrame,\n",
        "    use_coefficients: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete MRIA research study, including primary and robustness analyses.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point for the\n",
        "    entire project. It sequentially executes the main analysis pipeline (Task 8)\n",
        "    and the advanced robustness analysis suite (Task 9), governed by a single,\n",
        "    comprehensive configuration dictionary.\n",
        "\n",
        "    Args:\n",
        "        initial_production (pd.DataFrame): DataFrame of baseline production values.\n",
        "        initial_trade (pd.DataFrame): DataFrame of baseline inter-regional trade flows.\n",
        "        supply_coefficients (pd.DataFrame): DataFrame of supply technical coefficients.\n",
        "        use_coefficients (pd.DataFrame): DataFrame of use technical coefficients.\n",
        "        config (Dict[str, Any]): The master configuration dictionary that\n",
        "                                 governs all aspects of the experiment.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing two top-level keys:\n",
        "                        'main_analysis_results': The results from the primary\n",
        "                                                 Sensitivity, Criticality, and\n",
        "                                                 Incremental Disruption analyses.\n",
        "                        'robustness_analysis_results': The results from the\n",
        "                                                       suite of robustness tests.\n",
        "    \"\"\"\n",
        "    # Announce the start of the final, top-level execution.\n",
        "    print(\"\\n\\n##########################################################\")\n",
        "    print(\"######   STARTING COMPLETE MRIA EXPERIMENT PIPELINE   ######\")\n",
        "    print(\"##########################################################\")\n",
        "\n",
        "    # This dictionary will store all final results.\n",
        "    experiment_results: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # --- STAGE 1: EXECUTE THE MAIN ANALYSIS PIPELINE ---\n",
        "        # This runs the core analyses as defined in the paper (Sensitivity,\n",
        "        # Criticality, Incremental Disruption) based on the config.\n",
        "        print(\"\\n>>> STAGE 1: EXECUTING MAIN ANALYSIS PIPELINE...\")\n",
        "        main_pipeline_results = run_mria_pipeline(\n",
        "            initial_production,\n",
        "            initial_trade,\n",
        "            supply_coefficients,\n",
        "            use_coefficients,\n",
        "            config\n",
        "        )\n",
        "        # Store the collected results from the main pipeline.\n",
        "        experiment_results['main_analysis_results'] = main_pipeline_results\n",
        "        print(\">>> STAGE 1: MAIN ANALYSIS PIPELINE COMPLETED SUCCESSFULLY.\")\n",
        "\n",
        "        # --- STAGE 2: EXECUTE THE ROBUSTNESS ANALYSIS SUITE ---\n",
        "        # This runs the advanced robustness checks (Alpha, Disruption Mag, etc.)\n",
        "        # based on the 'robustness_analysis' section of the config.\n",
        "        print(\"\\n>>> STAGE 2: EXECUTING ROBUSTNESS ANALYSIS SUITE...\")\n",
        "        robustness_suite_results = run_robustness_analyses(\n",
        "            initial_production,\n",
        "            initial_trade,\n",
        "            supply_coefficients,\n",
        "            use_coefficients,\n",
        "            config\n",
        "        )\n",
        "        # Store the collected results from the robustness suite.\n",
        "        experiment_results['robustness_analysis_results'] = robustness_suite_results\n",
        "        print(\">>> STAGE 2: ROBUSTNESS ANALYSIS SUITE COMPLETED SUCCESSFULLY.\")\n",
        "\n",
        "        # Announce the successful completion of the entire experiment.\n",
        "        print(\"\\n\\n##########################################################\")\n",
        "        print(\"######    COMPLETE MRIA EXPERIMENT FINISHED SUCCESSFULLY    ######\")\n",
        "        print(\"##########################################################\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Provide a final, top-level catch for any unexpected errors.\n",
        "        print(\"\\n\\n##########################################################\")\n",
        "        print(\"######      !!! MRIA EXPERIMENT FAILED !!!      ######\")\n",
        "        print(\"##########################################################\")\n",
        "        print(f\"A critical error occurred during the full experiment: {e}\")\n",
        "        # Re-raise the exception to provide a full traceback for debugging.\n",
        "        raise\n",
        "\n",
        "    # Return the final, comprehensive dictionary of all results.\n",
        "    return experiment_results\n"
      ],
      "metadata": {
        "id": "OYoRjV71NNem"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}